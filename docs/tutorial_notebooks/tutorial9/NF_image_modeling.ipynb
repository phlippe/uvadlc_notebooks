{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 9.2: Normalizing Flows on image modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**: show application of NFs on image modeling, assumably on MNIST. Introduce Activation Normalization, 1x1 convolution, multi-scale architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "USE_NOTEBOOK = False\n",
    "\n",
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np \n",
    "import scipy.linalg\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "if USE_NOTEBOOK:\n",
    "    %matplotlib inline \n",
    "    from IPython.display import set_matplotlib_formats\n",
    "    set_matplotlib_formats('svg', 'pdf') # For export\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## Progress bar\n",
    "if USE_NOTEBOOK:\n",
    "    from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial9\"\n",
    "\n",
    "# Function for setting the seed\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Fetching the device that will be used throughout this notebook\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "def discretize(sample):\n",
    "    return (sample * 255).to(torch.int32)\n",
    "\n",
    "# Transformations applied on each image => only make them a tensor\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                discretize])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "train_dataset = MNIST(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [50000, 10000])\n",
    "\n",
    "# Loading the test set\n",
    "test_set = MNIST(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "# Note that for actually training a model, we will use different data loaders\n",
    "# with a lower batch size.\n",
    "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=False)\n",
    "val_loader = data.DataLoader(val_set, batch_size=64, shuffle=False, drop_last=False)\n",
    "test_loader = data.DataLoader(test_set, batch_size=64, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/pdf": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"184.544262pt\" version=\"1.1\" viewBox=\"0 0 349.2 184.544262\" width=\"349.2pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2020-07-31T17:49:29.022605</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.3.0, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 184.544262 \n",
       "L 349.2 184.544262 \n",
       "L 349.2 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g clip-path=\"url(#pe600ac01ae)\">\n",
       "    <image height=\"171\" id=\"image64e0411277\" transform=\"scale(1 -1)translate(0 -171)\" width=\"335\" x=\"7.2\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-6.344262\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pe600ac01ae\">\n",
       "   <rect height=\"170.144262\" width=\"334.8\" x=\"7.2\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if USE_NOTEBOOK:\n",
    "\n",
    "    def show_imgs(imgs):\n",
    "        imgs = torchvision.utils.make_grid(imgs, nrow=4, pad_value=128)\n",
    "        np_imgs = imgs.numpy()\n",
    "        sns.set_style(\"white\")\n",
    "        plt.imshow(np.transpose(np_imgs, (1,2,0)), interpolation='nearest')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    show_imgs([train_set[i][0] for i in range(8)])\n",
    "\n",
    "else:\n",
    "    def show_imgs(imgs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFlow(nn.Module):\n",
    "    \n",
    "    def __init__(self, flows):\n",
    "        super().__init__()\n",
    "        self.prior = torch.distributions.normal.Normal(loc=0.0, scale=1.0)\n",
    "        self.flows = nn.ModuleList(flows)\n",
    "        \n",
    "    def forward(self, imgs, return_nll=False):\n",
    "        z = imgs\n",
    "        ldj = torch.zeros(z.shape[0], device=device)\n",
    "        for flow in self.flows:\n",
    "            z, ldj = flow(z, ldj, reverse=False)\n",
    "        log_pz = self.prior.log_prob(z).sum(dim=[1,2,3])\n",
    "        log_px = ldj + log_pz\n",
    "        nll = -log_px\n",
    "        if return_nll:\n",
    "            return nll\n",
    "        else:\n",
    "            # Calculating bits per dimension\n",
    "            bpd = nll * np.log2(np.exp(1)) / np.prod(imgs.shape[1:])\n",
    "            return bpd\n",
    "        \n",
    "    def sample(self, img_shape):\n",
    "        z = self.prior.sample(sample_shape=img_shape).to(device)\n",
    "        ldj = torch.zeros(img_shape[0], device=device)\n",
    "        with torch.no_grad():\n",
    "            for flow in reversed(self.flows):\n",
    "                z, ldj = flow(z, ldj, reverse=True)\n",
    "        return z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dequantization(nn.Module):\n",
    "    \n",
    "    def __init__(self, alpha=1e-5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, z, ldj, reverse=False):\n",
    "        if not reverse:\n",
    "            z, ldj = self.dequant(z, ldj)\n",
    "            z, ldj = self.sigmoid(z, ldj, reverse=True)\n",
    "        else:\n",
    "            z, ldj = self.sigmoid(z, ldj, reverse=False)\n",
    "            z = z * 256\n",
    "            z = torch.floor(z).clamp(min=0, max=255).to(torch.int32)\n",
    "        return z, ldj\n",
    "    \n",
    "    def sigmoid(self, z, ldj, reverse=False):\n",
    "        if not reverse:\n",
    "            ldj += (-z-2*F.softplus(-z)).sum(dim=[1,2,3])\n",
    "            z = torch.sigmoid(z)\n",
    "        else:\n",
    "            z = z * (1 - self.alpha) + 0.5 * self.alpha\n",
    "            ldj += np.log(1 - self.alpha) * np.prod(z.shape[1:])\n",
    "            ldj += (-torch.log(z) - torch.log(1-z)).sum(dim=[1,2,3])\n",
    "            z = torch.log(z) - torch.log(1-z)\n",
    "        return z, ldj\n",
    "    \n",
    "    def dequant(self, z, ldj):\n",
    "        z = z.to(torch.float32) \n",
    "        z = z + torch.rand_like(z)\n",
    "        z = z / 256.0\n",
    "        ldj -= np.log(256.0) * np.prod(z.shape[1:])\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.2539])\n",
      "tensor([-2.2539])\n"
     ]
    }
   ],
   "source": [
    "z = torch.Tensor([2])\n",
    "print(-z-2*F.softplus(-z))\n",
    "z = torch.sigmoid(z)\n",
    "print(torch.log(z)+torch.log(1-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig tensor(0, dtype=torch.int32)\n",
      "Reconst tensor(1, dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "orig_img = train_set[0][0].unsqueeze(dim=0)\n",
    "ldj = torch.zeros(1,)\n",
    "dequant_module = Dequantization()\n",
    "deq_img, ldj = dequant_module(orig_img, ldj, reverse=False)\n",
    "reconst_img, ldj = dequant_module(deq_img, ldj, reverse=True)\n",
    "\n",
    "d1, d2 = torch.where(orig_img.squeeze() != reconst_img.squeeze())\n",
    "for i in range(d1.shape[0]):\n",
    "    print(\"Orig\", orig_img[0,0,d1[i], d2[i]])\n",
    "    print(\"Reconst\", reconst_img[0,0,d1[i], d2[i]])\n",
    "\n",
    "# Layer is not strictly invertible due to float precision constraints\n",
    "# assert (orig_img == reconst_img).all().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalDequantization(Dequantization):\n",
    "    \n",
    "    def __init__(self, var_flows, alpha=1e-5):\n",
    "        super().__init__(alpha=alpha)\n",
    "        self.flows = nn.ModuleList(var_flows)\n",
    "        \n",
    "    def dequant(self, z, ldj):\n",
    "        z = z.to(torch.float32)\n",
    "        img = (z / 255.0) * 2 - 1\n",
    "        deq_noise = torch.rand_like(z)\n",
    "        deq_noise, ldj = self.sigmoid(deq_noise, ldj, reverse=True)\n",
    "        for flow in self.flows:\n",
    "            deq_noise, ldj = flow(deq_noise, ldj, reverse=False, orig_img=img)\n",
    "        deq_noise, ldj = self.sigmoid(deq_noise, ldj, reverse=False)\n",
    "        z = (z + deq_noise) / 256.0\n",
    "        ldj -= np.log(256.0) * np.prod(z.shape[1:])\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouplingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, network, mask, c_in):\n",
    "        super().__init__()\n",
    "        self.register_buffer('mask', mask)\n",
    "        self.network = network\n",
    "        self.scaling_factor = nn.Parameter(torch.zeros(c_in))\n",
    "        \n",
    "        #self.network[-1].weight.data.zero_()\n",
    "        #self.network[-1].bias.data.zero_()\n",
    "    \n",
    "    def forward(self, z, ldj, reverse=False, orig_img=None):\n",
    "        z_in = z * self.mask\n",
    "        if orig_img is None:\n",
    "            nn_out = self.network(z_in)\n",
    "        else:\n",
    "            nn_out = self.network(torch.cat([z_in, orig_img], dim=1))\n",
    "        s, t = nn_out.chunk(2, dim=1)\n",
    "        ## Stabilize scaling output\n",
    "        s_fac = self.scaling_factor.exp().view(1, -1, 1, 1)\n",
    "        s = torch.tanh(s / s_fac) * s_fac\n",
    "        \n",
    "        s = s * (1 - self.mask)\n",
    "        t = t * (1 - self.mask)\n",
    "        \n",
    "        if not reverse:\n",
    "            z = (z + t) * torch.exp(s)\n",
    "            ldj += s.sum(dim=[1,2,3])\n",
    "        else:\n",
    "            z = (z * torch.exp(-s)) - t\n",
    "            ldj -= s.sum(dim=[1,2,3])\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkerboard_mask(h, w, invert=False):\n",
    "    x, y = torch.arange(h, dtype=torch.int32), torch.arange(w, dtype=torch.int32)\n",
    "    xx, yy = torch.meshgrid(x, y)\n",
    "    mask = torch.fmod(xx + yy, 2)\n",
    "    mask = mask.to(torch.float32).view(1, 1, h, w)\n",
    "    if invert:\n",
    "        mask = 1 - mask\n",
    "    return mask\n",
    "\n",
    "def create_channel_mask(c_in, invert=False):\n",
    "    mask = torch.cat([torch.ones(c_in//2, dtype=torch.float32), \n",
    "                      torch.zeros(c_in-c_in//2, dtype=torch.float32)])\n",
    "    mask = mask.view(1, c_in, 1, 1)\n",
    "    if invert:\n",
    "        mask = 1 - mask\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_flow(flow, max_epochs=80, sample_shape=(8,1,28,28), model_name=\"MNISTFlow\"):\n",
    "    optimizer = optim.Adam(flow.parameters(), lr=1e-3)\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.98) # Every epoch, we multiply the LR by 0.95\n",
    "    train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True)\n",
    "    \n",
    "    print(\"Start training %s...\" % model_name)\n",
    "    \n",
    "    avg_bpd = 8.0\n",
    "    \n",
    "    val_scores = []\n",
    "    for epoch in range(max_epochs):\n",
    "        pbar = tqdm(train_loader, leave=False) if USE_NOTEBOOK else train_loader\n",
    "        for imgs, _ in pbar:\n",
    "            imgs = imgs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            bpd = flow(imgs).mean()\n",
    "            bpd.backward()\n",
    "            nn.utils.clip_grad_norm_(flow.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_bpd = 0.9 * avg_bpd + 0.1 * bpd.item()\n",
    "            if USE_NOTEBOOK:\n",
    "                pbar.set_description(\"[Epoch %i] Bits per dimension: %5.3fbpd\" % (epoch+1, avg_bpd))\n",
    "        lr_scheduler.step()\n",
    "        if not USE_NOTEBOOK:\n",
    "            print(\"Training bpd: %5.3fbpd\" % avg_bpd)\n",
    "        val_bpd = test_flow(flow, val_loader)\n",
    "        print(\"[Epoch %2i] Validation bits per dimension: %5.3fbpd\" % (epoch+1, val_bpd))\n",
    "        \n",
    "        if len(val_scores) == 0 or val_bpd < min(val_scores):\n",
    "            print(\"\\t   (New best performance, saving model...)\")\n",
    "            save_model(flow, CHECKPOINT_PATH, model_name)\n",
    "        val_scores.append(val_bpd)\n",
    "        \n",
    "        if (epoch+1) % 5 == 0:\n",
    "            samples = flow.sample(img_shape=sample_shape)\n",
    "            show_imgs(samples.cpu())\n",
    "            \n",
    "    if USE_NOTEBOOK:\n",
    "        # Plot a curve of the validation accuracy\n",
    "        sns.set()\n",
    "        plt.plot([i for i in range(1,len(val_scores)+1)], val_scores)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Validation bits per dimension\")\n",
    "        plt.title(\"Validation performance of %s\" % model_name)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"Val scores\", val_scores)\n",
    "    \n",
    "    test_bpd = test_flow(flow, test_loader)\n",
    "    print(\"Test bits per dimension: %5.3fbpd\" % (test_bpd))\n",
    "    \n",
    "def test_flow(flow, data_loader, import_samples=8):\n",
    "    flow.eval()\n",
    "    test_bpd, counter = 0.0, 0\n",
    "    for imgs, _ in (tqdm(data_loader, leave=False, desc=\"Testing...\") if USE_NOTEBOOK else data_loader):\n",
    "        imgs = imgs.to(device)\n",
    "        with torch.no_grad():\n",
    "            samples = []\n",
    "            for _ in range(import_samples):\n",
    "                img_nll = flow(imgs, return_nll=True)\n",
    "                samples.append(img_nll)\n",
    "            img_nll = torch.stack(samples, dim=-1)\n",
    "            img_nll = torch.logsumexp(img_nll, dim=-1) - np.log(import_samples)\n",
    "            img_bpd = img_nll * np.log2(np.exp(1)) / np.prod(imgs.shape[1:])\n",
    "            \n",
    "            test_bpd += img_bpd.sum().item()\n",
    "            counter += img_bpd.shape[0]\n",
    "    test_bpd = test_bpd / counter\n",
    "    return test_bpd \n",
    "\n",
    "def save_model(model, model_path, model_name):\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(model_path, model_name + \".tar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, c_in, c_hidden=32, c_out=-1):\n",
    "        super().__init__()\n",
    "        c_out = c_out if c_out > 0 else 2 * c_in\n",
    "        self.nn = nn.Sequential(\n",
    "                        nn.Conv2d(c_in, c_hidden, 3, padding=1),\n",
    "                        nn.GELU(),\n",
    "                        nn.Conv2d(c_hidden, c_hidden, 3, padding=1),\n",
    "                        nn.GELU(),\n",
    "                        nn.Conv2d(c_hidden, c_hidden, 3, padding=1),\n",
    "                        nn.GELU(),\n",
    "                        nn.Conv2d(c_hidden, c_out, 3, padding=1)\n",
    "                     )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.nn(x)\n",
    "    \n",
    "\n",
    "class GatedConv(nn.Module):\n",
    "    def __init__(self, c_in, c_hidden):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(c_in, c_hidden, 3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(c_hidden, 2*c_in, 3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        val, gate = out.chunk(2, dim=1)\n",
    "        return x + val * torch.sigmoid(gate)\n",
    "\n",
    "class GatedConvNet(nn.Module):\n",
    "    def __init__(self, c_in, height, width, c_hidden=32, c_out=-1, num_layers=4):\n",
    "        super().__init__()\n",
    "        c_out = c_out if c_out > 0 else 2 * c_in\n",
    "        layers = []\n",
    "        layers += [nn.Conv2d(c_in, c_hidden, 3, padding=1)]\n",
    "        for _ in range(num_layers):\n",
    "            layers += [GatedConv(c_hidden, c_hidden),\n",
    "                       nn.LayerNorm([c_hidden, height, width])]\n",
    "        layers += [nn.Conv2d(c_hidden, c_out, 3, padding=1)]\n",
    "        self.nn = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.nn(x)\n",
    "    \n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, n_hidden=1024):\n",
    "        super().__init__()\n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Linear(28*28, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, 28*28*2)\n",
    "        )\n",
    "        self.nn[-1].weight.data.zero_()\n",
    "        self.nn[-1].bias.data.zero_()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[2], x.shape[3]\n",
    "        out = self.nn(x.view(x.shape[0], -1))\n",
    "        return out.reshape(-1, 2, h, w)\n",
    "    \n",
    "class ConvEncDecNet(nn.Module):\n",
    "    def __init__(self, c_in, c_hidden=32):\n",
    "        super().__init__()\n",
    "        self.enc0 = nn.Sequential(\n",
    "            nn.Conv2d(c_in, c_hidden, 3, stride=1, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(c_hidden, c_hidden, 3, stride=2, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(c_hidden, c_hidden, 3, stride=1, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(c_hidden, c_hidden*2, 3, stride=2, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(c_hidden*2, c_hidden*2, 3, stride=1, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(c_hidden*2, c_hidden, 3, stride=2, output_padding=1, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(c_hidden*2, c_hidden, 1, stride=1, padding=0),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(c_hidden, c_hidden, 3, stride=2, output_padding=1, padding=1),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.dec0 = nn.Sequential(\n",
    "            nn.Conv2d(c_hidden*2, c_hidden, 1, stride=1, padding=0),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(c_hidden, 2*c_in, 3, stride=1, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_enc0 = self.enc0(x)\n",
    "        x_enc1 = self.enc1(x_enc0)\n",
    "        x_enc2 = self.enc2(x_enc1)\n",
    "        x_dec2 = self.dec2(x_enc2)\n",
    "        x_dec1 = self.dec1(torch.cat([x_dec2, x_enc1], dim=1))\n",
    "        x_dec0 = self.dec0(torch.cat([x_dec1, x_enc0], dim=1))\n",
    "        return x_dec0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_simple_flow(use_vardeq=True):\n",
    "    set_seed(42)\n",
    "    flow_layers = []\n",
    "    if use_vardeq:\n",
    "        vardeq_layers = [CouplingLayer(network=ConvNet(c_in=2,c_out=2),\n",
    "                                       mask=create_checkerboard_mask(h=28, w=28, invert=(i%2==1)),\n",
    "                                       c_in=1) for i in range(1)]\n",
    "        flow_layers += [VariationalDequantization(var_flows=vardeq_layers)]\n",
    "    else:\n",
    "        flow_layers += [Dequantization()]\n",
    "    for i in range(8):\n",
    "        flow_layers += [CouplingLayer(network=ConvNet(c_in=1),\n",
    "                                      mask=create_checkerboard_mask(h=28, w=28, invert=(i%2==1)),\n",
    "                                      c_in=1)]\n",
    "    flow_model = ImageFlow(flow_layers).to(device)\n",
    "    return flow_model\n",
    "# train_flow(flow_model, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-scale architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeFlow(nn.Module):\n",
    "    \n",
    "    def forward(self, z, ldj, reverse=False):\n",
    "        B, C, H, W = z.shape\n",
    "        if not reverse:\n",
    "            z = z.reshape(B, C, H//2, 2, W//2, 2)\n",
    "            z = z.permute(0, 1, 3, 5, 2, 4)\n",
    "            z = z.reshape(B, 4*C, H//2, W//2)\n",
    "        else:\n",
    "            z = z.reshape(B, C//4, 2, 2, H, W)\n",
    "            z = z.permute(0, 1, 4, 2, 5, 3)\n",
    "            z = z.reshape(B, C//4, H*2, W*2)\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image (before) tensor([[[[ 0,  1,  2,  3],\n",
      "          [ 4,  5,  6,  7],\n",
      "          [ 8,  9, 10, 11],\n",
      "          [12, 13, 14, 15]]]])\n",
      "Image (forward) tensor([[[[ 0,  2],\n",
      "          [ 8, 10]],\n",
      "\n",
      "         [[ 1,  3],\n",
      "          [ 9, 11]],\n",
      "\n",
      "         [[ 4,  6],\n",
      "          [12, 14]],\n",
      "\n",
      "         [[ 5,  7],\n",
      "          [13, 15]]]])\n",
      "Image (reverse) tensor([[[[ 0,  1,  2,  3],\n",
      "          [ 4,  5,  6,  7],\n",
      "          [ 8,  9, 10, 11],\n",
      "          [12, 13, 14, 15]]]])\n"
     ]
    }
   ],
   "source": [
    "sq_flow = SqueezeFlow()\n",
    "rand_img = torch.arange(16).view(1, 1, 4, 4)\n",
    "print(\"Image (before)\", rand_img)\n",
    "forward_img, _ = sq_flow(rand_img, ldj=None, reverse=False)\n",
    "print(\"Image (forward)\", forward_img)\n",
    "reconst_img, _ = sq_flow(forward_img, ldj=None, reverse=True)\n",
    "print(\"Image (reverse)\", reconst_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitFlow(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prior = torch.distributions.normal.Normal(loc=0.0, scale=1.0)\n",
    "    \n",
    "    def forward(self, z, ldj, reverse=False):\n",
    "        if not reverse:\n",
    "            z, z_split = z.chunk(2, dim=1)\n",
    "            ldj += self.prior.log_prob(z_split).sum(dim=[1,2,3])\n",
    "        else:\n",
    "            z_split = self.prior.sample(sample_shape=z.shape).to(device)\n",
    "            z = torch.cat([z, z_split], dim=1)\n",
    "            ldj -= self.prior.log_prob(z_split).sum(dim=[1,2,3])\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Invertible1x1Conv(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in):\n",
    "        super().__init__()\n",
    "        w_init = np.random.randn(c_in, c_in)\n",
    "        w_init = np.linalg.qr(w_init)[0].astype(np.float32)\n",
    "        self.weight = nn.Parameter(torch.from_numpy(w_init))\n",
    "    \n",
    "    def forward(self, z, ldj, reverse=False):\n",
    "        weight_ldj = torch.slogdet(self.weight)[1]\n",
    "        weight_ldj = weight_ldj * z.shape[2] * z.shape[3]\n",
    "        if not reverse:\n",
    "            w = self.weight\n",
    "            ldj += weight_ldj\n",
    "        else:\n",
    "            w = torch.inverse(self.weight.double()).float()\n",
    "            ldj -= weight_ldj\n",
    "        \n",
    "        z = F.conv2d(z, w[:,:,None,None])\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Invertible1x1ConvLU(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in):\n",
    "        super().__init__()\n",
    "        w_init = np.random.randn(c_in, c_in)\n",
    "        w_init = np.linalg.qr(w_init)[0].astype(np.float32)\n",
    "        \n",
    "        # LU decomposition can slightly speed up the inverse\n",
    "        np_p, np_l, np_u = scipy.linalg.lu(w_init)\n",
    "        np_s = np.diag(np_u)\n",
    "        np_sign_s = np.sign(np_s)\n",
    "        np_log_s = np.log(np.abs(np_s))\n",
    "        np_u = np.triu(np_u, k=1)\n",
    "        l_mask = np.tril(np.ones(w_init.shape, dtype=np.float32), -1)\n",
    "        eye = np.eye(*w_init.shape, dtype=np.float32)\n",
    "\n",
    "        self.register_buffer('p', torch.Tensor(np_p.astype(np.float32)))\n",
    "        self.register_buffer('sign_s', torch.Tensor(np_sign_s.astype(np.float32)))\n",
    "        self.l = nn.Parameter(torch.Tensor(np_l.astype(np.float32)), requires_grad=True)\n",
    "        self.log_s = nn.Parameter(torch.Tensor(np_log_s.astype(np.float32)), requires_grad=True)\n",
    "        self.u = nn.Parameter(torch.Tensor(np_u.astype(np.float32)), requires_grad=True)\n",
    "        self.register_buffer('l_mask', torch.Tensor(l_mask))\n",
    "        self.register_buffer('eye', torch.Tensor(eye))\n",
    "    \n",
    "    def forward(self, z, ldj, reverse=False):\n",
    "        l, log_s, u = self.l, self.log_s, self.u\n",
    "        l = l * self.l_mask + self.eye\n",
    "        u = u * self.l_mask.transpose(0, 1).contiguous() + torch.diag(self.sign_s * torch.exp(log_s))\n",
    "        w = torch.matmul(self.p, torch.matmul(l, u))\n",
    "        weight_ldj = log_s.sum()\n",
    "        weight_ldj = weight_ldj * z.shape[2] * z.shape[3]\n",
    "        if not reverse:\n",
    "            ldj += weight_ldj\n",
    "        else:\n",
    "            w = torch.inverse(w.double()).float()\n",
    "            ldj -= weight_ldj\n",
    "        \n",
    "        z = F.conv2d(z, w[:,:,None,None])\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiscale_flow(use_vardeq=True, use_1x1_convs=True):\n",
    "    set_seed(42)\n",
    "    flow_layers = []\n",
    "    if use_vardeq:\n",
    "        vardeq_layers = [CouplingLayer(network=ConvNet(c_in=2,c_out=2),\n",
    "                                       mask=create_checkerboard_mask(h=28, w=28, invert=(i%2==1)),\n",
    "                                       c_in=1) for i in range(4)]\n",
    "        flow_layers += [VariationalDequantization(vardeq_layers)]\n",
    "    else:\n",
    "        flow_layers += [Dequantization()]\n",
    "    \n",
    "    flow_layers += [CouplingLayer(network=ConvNet(c_in=1),\n",
    "                                  mask=create_checkerboard_mask(h=28, w=28, invert=(i%2==1)),\n",
    "                                  c_in=1) for i in range(2)]\n",
    "    flow_layers += [SqueezeFlow()]\n",
    "    for i in range(2):\n",
    "        if use_1x1_convs:\n",
    "            flow_layers += [Invertible1x1ConvLU(c_in=4)]\n",
    "        flow_layers += [CouplingLayer(network=ConvNet(c_in=4, c_hidden=48),\n",
    "                                      mask=create_channel_mask(c_in=4, invert=(i%2==1)),\n",
    "                                      c_in=4)]\n",
    "    if use_1x1_convs:\n",
    "        flow_layers += [Invertible1x1ConvLU(c_in=4)]\n",
    "    flow_layers += [Invertible1x1ConvLU(c_in=4),\n",
    "                    SplitFlow(),\n",
    "                    SqueezeFlow()]\n",
    "    for i in range(4):\n",
    "        if use_1x1_convs:\n",
    "            flow_layers += [Invertible1x1ConvLU(c_in=8)]\n",
    "        flow_layers += [CouplingLayer(network=ConvNet(c_in=8, c_hidden=64),\n",
    "                                      mask=create_channel_mask(c_in=8, invert=(i%2==1)),\n",
    "                                      c_in=8)]\n",
    "\n",
    "\n",
    "    flow_model = ImageFlow(flow_layers).to(device)\n",
    "    return flow_model\n",
    "# train_flow(flow_model, sample_shape=[8,8,7,7], model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_simple_flow(use_vardeq=False, model_name=\"MNISTFlow_simple\")\n",
    "# train_simple_flow(use_vardeq=True, model_name=\"MNISTFlow_vardeq\")\n",
    "# train_multiscale_flow(use_1x1_convs=False, model_name=\"MNISTFlow_multiscale\")\n",
    "# train_multiscale_flow(use_1x1_convs=True, model_name=\"MNISTFlow_multiscale_1x1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flow_model_simple = create_simple_flow(use_vardeq=True)\n",
    "flow_model_simple.load_state_dict(torch.load(os.path.join(CHECKPOINT_PATH, \"MNISTFlow_vardeq.tar\")))\n",
    "samples = flow_model_simple.sample(img_shape=[16,1,28,28])\n",
    "show_imgs(samples.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flow_model_multiscale = create_multiscale_flow(use_vardeq=True, use_1x1_convs=True)\n",
    "flow_model_multiscale.load_state_dict(torch.load(os.path.join(CHECKPOINT_PATH, \"MNISTFlow_multiscale_1x1.tar\")))\n",
    "set_seed(45)\n",
    "samples = flow_model_multiscale.sample(img_shape=[16,8,7,7])\n",
    "show_imgs(samples.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training MNISTFlow_multiscale_gated...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=390.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing...', max=157.0, style=ProgressStyle(description_w"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1] Validation bits per dimension: 1.277bpd\n",
      "\t   (New best performance, saving model...)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803fe1ce1f2848a08e0dbe2644523614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=390.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-7252b10f53f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mflow_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageFlow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflow_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mflow_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtrain_flow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_multiscale_flow3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"MNISTFlow_multiscale_gated\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-e15c59ec8939>\u001b[0m in \u001b[0;36mtrain_flow\u001b[0;34m(flow, max_epochs, sample_shape, model_name)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mUSE_NOTEBOOK\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp1/lib/python3.7/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp1/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp1/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp1/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp1/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp1/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp1/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp1/lib/python3.7/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp1/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2768\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2770\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp1/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2703\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2704\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_MAPMODES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2705\u001b[0;31m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2706\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2707\u001b[0m             \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadonly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp1/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mnew\u001b[0;34m(mode, size, color)\u001b[0m\n\u001b[1;32m   2604\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageColor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2606\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2608\u001b[0m         \u001b[0;31m# RGB or RGBA value for a P image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp1/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# FIXME: turn mode and size into delegating properties?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def create_multiscale_flow3(use_vardeq=True, use_1x1_convs=True):\n",
    "    set_seed(42)\n",
    "    flow_layers = []\n",
    "    if use_vardeq:\n",
    "        vardeq_layers = [CouplingLayer(network=ConvNet(c_in=2,c_out=2),\n",
    "                                       mask=create_checkerboard_mask(h=28, w=28, invert=(i%2==1)),\n",
    "                                       c_in=1) for i in range(4)]\n",
    "        flow_layers += [VariationalDequantization(vardeq_layers)]\n",
    "    else:\n",
    "        flow_layers += [Dequantization()]\n",
    "    \n",
    "    flow_layers += [CouplingLayer(network=ConvNet(c_in=1),\n",
    "                                  mask=create_checkerboard_mask(h=28, w=28, invert=(i%2==1)),\n",
    "                                  c_in=1) for i in range(2)]\n",
    "    flow_layers += [SqueezeFlow()]\n",
    "    for i in range(4):\n",
    "        if use_1x1_convs:\n",
    "            flow_layers += [Invertible1x1ConvLU(c_in=4)]\n",
    "        flow_layers += [CouplingLayer(network=ConvNet(c_in=4, c_hidden=64),\n",
    "                                      mask=create_channel_mask(c_in=4, invert=(i%2==1)),\n",
    "                                      c_in=4)]\n",
    "    if use_1x1_convs:\n",
    "        flow_layers += [Invertible1x1ConvLU(c_in=4)]\n",
    "    flow_layers += [Invertible1x1ConvLU(c_in=4),\n",
    "                    SplitFlow(),\n",
    "                    SqueezeFlow()]\n",
    "    for i in range(6):\n",
    "        if use_1x1_convs:\n",
    "            flow_layers += [Invertible1x1ConvLU(c_in=8)]\n",
    "        flow_layers += [CouplingLayer(network=ConvNet(c_in=8, c_hidden=96),\n",
    "                                      mask=create_channel_mask(c_in=8, invert=(i%2==1)),\n",
    "                                      c_in=8)]\n",
    "\n",
    "\n",
    "    flow_model = ImageFlow(flow_layers).to(device)\n",
    "    return flow_model\n",
    "train_flow(create_multiscale_flow3(), sample_shape=[8,8,7,7], model_name=\"MNISTFlow_multiscale_deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training MNISTFlow_multiscale_gated...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06b22231d244191b4fc4cbaed328a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=390.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-45a06e4cfcdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mflow_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageFlow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflow_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mflow_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtrain_flow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_multiscale_flow2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"MNISTFlow_multiscale_gated\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-4a08aa1ca675>\u001b[0m in \u001b[0;36mtrain_flow\u001b[0;34m(flow, max_epochs, sample_shape, model_name)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mbpd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mbpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp1/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp1/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def create_multiscale_flow2(use_vardeq=True, use_1x1_convs=True):\n",
    "    set_seed(42)\n",
    "    flow_layers = []\n",
    "    if use_vardeq:\n",
    "        vardeq_layers = [CouplingLayer(network=GatedConvNet(c_in=2, c_out=2, height=28, width=28),\n",
    "                                       mask=create_checkerboard_mask(h=28, w=28, invert=(i%2==1)),\n",
    "                                       c_in=1) for i in range(4)]\n",
    "        flow_layers += [VariationalDequantization(vardeq_layers)]\n",
    "    else:\n",
    "        flow_layers += [Dequantization()]\n",
    "    \n",
    "    flow_layers += [CouplingLayer(network=GatedConvNet(c_in=1, height=28, width=28),\n",
    "                                  mask=create_checkerboard_mask(h=28, w=28, invert=(i%2==1)),\n",
    "                                  c_in=1) for i in range(2)]\n",
    "    flow_layers += [SqueezeFlow()]\n",
    "    for i in range(2):\n",
    "        if use_1x1_convs:\n",
    "            flow_layers += [Invertible1x1ConvLU(c_in=4)]\n",
    "        flow_layers += [CouplingLayer(network=GatedConvNet(c_in=4, c_hidden=48, height=14, width=14),\n",
    "                                      mask=create_channel_mask(c_in=4, invert=(i%2==1)),\n",
    "                                      c_in=4)]\n",
    "    if use_1x1_convs:\n",
    "        flow_layers += [Invertible1x1ConvLU(c_in=4)]\n",
    "    flow_layers += [Invertible1x1ConvLU(c_in=4),\n",
    "                    SplitFlow(),\n",
    "                    SqueezeFlow()]\n",
    "    for i in range(4):\n",
    "        if use_1x1_convs:\n",
    "            flow_layers += [Invertible1x1ConvLU(c_in=8)]\n",
    "        flow_layers += [CouplingLayer(network=GatedConvNet(c_in=8, c_hidden=64, height=7, width=7),\n",
    "                                      mask=create_channel_mask(c_in=8, invert=(i%2==1)),\n",
    "                                      c_in=8)]\n",
    "\n",
    "\n",
    "    flow_model = ImageFlow(flow_layers).to(device)\n",
    "    return flow_model\n",
    "train_flow(create_multiscale_flow2(), sample_shape=[8,8,7,7], model_name=\"MNISTFlow_multiscale_gated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

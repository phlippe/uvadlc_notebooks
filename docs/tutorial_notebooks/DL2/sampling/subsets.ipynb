{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SGA - Sampling Subsets with Gumbel-Top $k$ Relaxations\n",
    "\n",
    "**Notebook:** \n",
    "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/sampling/subsets.ipynb)\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/sampling/subsets.ipynb)  \n",
    "**Author:**\n",
    "Adeel Pervez\n",
    "\n",
    "In this part we show how to include a subset sampling component in differentiable models using Gumbel Top $k$ relaxations.\n",
    "First we show how to build a differentiable subset sampler and then we show one application to differentiable $k$ nearest neighbor classification.\n",
    "\n",
    "Formally speaking we are given $N$ elements with weights $w_i$.\n",
    "We would like to sample $k$ elements from $N$ without replacement.\n",
    "Stated otherwise, we want a $k$-element subset $S=\\{w_{i_1}, w_{i_2},\\ldots, w_{i_k}\\}$ from $N$ elements.\n",
    "\n",
    "Given total weight $Z=\\sum w_i$, the first element is sampled with probability $\\frac{w_{i_1}}{Z}$, the second with probability $\\frac{w_{i_2}}{Z-w_{i_1}}$ and so on for $k$ elements.\n",
    "Multiplying the factors gives the following distribution for $k$ element subsets.\n",
    "\n",
    "$$ p(S) = \\frac{w_{i_1}}{Z}  \\frac{w_{i_2}}{Z-w_{i_1}}\\cdots \\frac{w_{i_k}}{Z-\\sum_{j=1}^{k-1} w_{i_j}}.$$\n",
    "\n",
    "In the introduction we showed how sampling from a categorical distribution could be recast as choosing the argmax of a set of Gumbel random variables.\n",
    "Relaxing the argmax with a softmax allowed us to approximate sampling from the target categorical distribution. \n",
    "A temperature could be used to control the extent of relaxation.\n",
    "In this case the the categorical probabilities are given by the softmax distribution\n",
    "$$p_i = \\frac{exp(x_i)}{\\sum_j exp(x_j)} = \\frac{w_i}{\\sum_j w_j}$$\n",
    "\n",
    "It turns out that by selecting the $k$ largest Gumbel random variables instead of just the largest we can sample subsets according to the sampling without replacement probability given above.\n",
    "This procedure is closely related to a procedure known by the name of weighted reservoir sampling.\n",
    "\n",
    "Seen this way, the Gumbel-Argmax trick is a method for sampling subsets of size $k=1$ with probabilities given by $p_i$.\n",
    "Replacing the argmax by a Top-$k$ procedure for selecting the $k$ largest elements generalizes the Gumbel-Argmax to sample size $k$ subsets with probability $p(S)$.\n",
    "In this case we think of the Top-$k$ procedure as returning a $k$-hot vector $y$ where $y_i=1$ if the $i$th element is selected and $y_i=0$ otherwise.\n",
    "Thus we represent subsets as $k$-hot vectors which also generalizes the representation of categorical samples as 1-hot vectors.\n",
    "\n",
    "The unrelaxed subset sampling procedure can then be written as follows given non-negative weights $w_i$.\n",
    "\n",
    "1. Compute keys $\\hat{r_i} = -\\log(-\\log(u_i)) +  \\log(w_i)$ for all $i$ and $u_i \\in U(0,1)$.\n",
    "2. Return $k$ largest keys $\\hat{r_i}$.\n",
    "\n",
    "## Top $k$ Relaxation\n",
    "\n",
    "We can construct an unrelaxed Top $k$ by iteratively applying the softmax $k$ times and sampling a 1-hot categorical sample at each step.\n",
    "The $k$ 1-hot categorical samples are then combined into a single $k$-vector.\n",
    "When the categorical sample gives a particular element, the log probability for that element is set to $-\\infty$ for the future iterations so that the element is never chosen again. We can relax this procedure by replacing samples from the softmax by the probabilities computed by softmax. When the softmax temperature is set to be small, the sampled and the relaxed outputs are close.\n",
    "\n",
    "In more detail the procedure is as follows.\n",
    "\n",
    "### Unrelaxed Version\n",
    "For $i=1\\ldots n$ and $j=1\\ldots k$, set $ \\alpha^1_i = \\hat{r_i}$ and $\\alpha_i^{j+1} = \\alpha_i^{j} + \\log(1-a_i^j)$\n",
    "\n",
    "Here $a^j_i$ is a sample the categorical distribution with probabilities $p(a^j_i = 1) = \\frac{\\exp(\\alpha_i^{j}/\\tau)}{\\sum_k\\exp(\\alpha_k^{j}/\\tau)}$ and $\\tau$ is a temperature.\n",
    "\n",
    "Note that when $a_i^j$ is a 1-hot categorical sample the $\\log(1-a_i^j)$ term in the first equation above sets the next $\\alpha_i^{j+1}$ to $-\\infty$ if $a_i^j=1$ and leaves it unchanged otherwise.\n",
    "This ensures that the $i$th element once sampled is not sampled in the next steps.\n",
    "Finally we add all the $k$ vectors as $\\sum_j a^j$ and return the output as the sample.\n",
    "\n",
    "\n",
    "### Relaxed Version\n",
    "To relax the above procedure we can replace the categorical sample at step by its expectation.\n",
    "In this case the update becomes\n",
    "\n",
    "For $i=1\\ldots n$ and $j=1\\ldots k$, set $ \\alpha^1_i = \\hat{r_i}$ and $\\alpha_i^{j+1} = \\alpha_i^{j} + \\log(1-p(a_i^j=1))$\n",
    "\n",
    "where $p(a^j_i = 1) = \\frac{\\exp(\\alpha_i^{j}/\\tau)}{\\sum_k\\exp(\\alpha_k^{j}/\\tau)}$ as above.\n",
    "At low values of $\\tau$ the softmax distribution becomes close to deterministic outputs a value that is close to $k$-hot.\n",
    "The temperature variable is a hyperparameter and ideally should be annealed from larger to smaller values during the course of training.\n",
    "However, in most applications the temperature is left fixed per trial and tuned using cross validation.\n",
    "Proper tuning of temperature can have a significant effect on the performance of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we use code from [[here](https://github.com/ermongroup/subsets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, ConcatDataset, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Subset Sampler Class\n",
    "\n",
    "The following `SubsetOperator` class implements the relaxed subset sampling procedure described above.\n",
    "As described the `forward` method takes a list of scores (unormalized log probs) of some fixed dimension.\n",
    "We add Gumbel noise with location 0 and scale 1 and divide by the temperature.\n",
    "Next we apply the Top-$k$ relaxation and return the resulting $k$-hot vector as the sampled subset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = np.finfo(np.float32).tiny\n",
    "\n",
    "class SubsetOperator(torch.nn.Module):\n",
    "    def __init__(self, k, tau=1.0, hard=False):\n",
    "        super(SubsetOperator, self).__init__()\n",
    "        self.k = k\n",
    "        self.hard = hard\n",
    "        self.tau = tau\n",
    "\n",
    "    def forward(self, scores):\n",
    "        m = torch.distributions.gumbel.Gumbel(torch.zeros_like(scores), torch.ones_like(scores))\n",
    "        g = m.sample()\n",
    "        scores = scores + g\n",
    "\n",
    "        # continuous top k\n",
    "        khot = torch.zeros_like(scores)\n",
    "        onehot_approx = torch.zeros_like(scores)\n",
    "        for i in range(self.k):\n",
    "            khot_mask = torch.max(1.0 - onehot_approx, torch.tensor([EPSILON]).cuda())\n",
    "            scores = scores + torch.log(khot_mask)\n",
    "            onehot_approx = torch.nn.functional.softmax(scores / self.tau, dim=1)\n",
    "            khot = khot + onehot_approx\n",
    "\n",
    "        if self.hard:\n",
    "            # straight through\n",
    "            khot_hard = torch.zeros_like(khot)\n",
    "            val, ind = torch.topk(khot, self.k, dim=1)\n",
    "            khot_hard = khot_hard.scatter_(1, ind, 1)\n",
    "            res = khot_hard - khot.detach() + khot\n",
    "        else:\n",
    "            res = khot\n",
    "\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can try the sampler on some example input and various temperatures. Note that the sum of the vectors elements is always $k$.\n",
    "At lower temperatures the output should be close to $k$-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0236, 0.5672, 0.4200, 0.9892]], device='cuda:0') tensor(2., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sampler = SubsetOperator(k=2, tau=1.0)\n",
    "\n",
    "x = torch.tensor([[1.,2.,3.,4.]]).to(gpu)\n",
    "y = sampler(x)\n",
    "print(y, y.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Sampling Distribution\n",
    "\n",
    "We empirically confirm that the $k$-hot relaxation generates subsets with the same distribution as the sampling without replacement distribution.\n",
    "For this we define a set with weights in `[1,2,3,4]` and generate 10000 subsets of size 2 using the true distribution (here with Gumbel Top $k$).\n",
    "Then we generate subsets using the relaxation given above with a fixed temperature.\n",
    "The samples are plotted side-by-side in a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor([[1.,2.,3.,4.]]).to(gpu)\n",
    "w = w.tile((10000,1))\n",
    "w_scores = torch.log(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Gumbel-Top-$k$ to get true distribution samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#true samples\n",
    "m = torch.distributions.gumbel.Gumbel(torch.zeros_like(w_scores), torch.ones_like(w_scores))\n",
    "g = m.sample()\n",
    "scores = w_scores + g\n",
    "samples = torch.topk(scores, 2)[1]\n",
    "samples = samples.detach().cpu().numpy()\n",
    "samples = [str(x) for x in samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get samples of subsets from the relaxation, we first apply the relaxation and choose the Top $k$ indices as the chosen subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relaxed samples\n",
    "r_samples = sampler(w_scores)\n",
    "\n",
    "r_samples = torch.topk(r_samples, 2)[1]\n",
    "r_samples = r_samples.detach().cpu().numpy()\n",
    "r_samples = [str(x) for x in r_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([samples,r_samples], align='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Differentiable k Nearest Neighbors\n",
    "\n",
    "Now we apply the subset sampling procedure to a classification problem with differentiable $k$ nearest neighbors. \n",
    "Recall that in $k$ nearest neighbors classification we look at the labels of the $k$ nearest neighbors and take the majority vote for classification.\n",
    "Unlike the classical form of nearest neighbors, we want to take the feature from a deep network.\n",
    "The $k$ nearest neighbors loss is cannot be directly used in differentiable models so we relax it with our subset relaxation.\n",
    "Furthermore instead of looking for the nearest neighbors in the entire dataset (which can be large) we choose a random subset of data points for the distance calculations.\n",
    "\n",
    "Given a query vector $q$ and a set $N$ of neighbors we compute the Euclidean distance between the $q$ and each element $i \\in N$.\n",
    "This gives us a list of scores (negative of the distances) and sample a $k$ size subset of these scores as a relaxed $k$-hot vector.\n",
    "\n",
    "Since this is a classification problem, during training we have the label $y$ for the query vector $q$ and for each of the neighbors $y_i$.\n",
    "If the labels are equal for a query, neighbor pair, we include the corresponding score otherwise we set it to 0 and take the sum to compute the loss.\n",
    "\n",
    "Given a subset of $k$ neighbors the loss can be written as \n",
    "$$L(S;q) = -\\sum_{j\\in S} I[y==y_i].$$ \n",
    "\n",
    "The actual loss is then the expectation of this expression over all subsets.\n",
    "$$ L(q) = E_S[L(S;q)].$$\n",
    "\n",
    "The `SubsetsDKNN` class computes the scores between the queries and neighbors and returns the sampled subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubsetsDKNN(torch.nn.Module):\n",
    "    def __init__(self, k, tau=1.0, hard=False, num_samples=-1):\n",
    "        super(SubsetsDKNN, self).__init__()\n",
    "        self.k = k\n",
    "        self.subset_sample = SubsetOperator(k=k, tau=tau, hard=hard)\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    # query: batch_size x p\n",
    "    # neighbors: 10k x p\n",
    "    def forward(self, query, neighbors, tau=1.0):\n",
    "        diffs = (query.unsqueeze(1) - neighbors.unsqueeze(0))\n",
    "        squared_diffs = diffs ** 2\n",
    "        l2_norms = squared_diffs.sum(2)\n",
    "        norms = l2_norms  # .sqrt() # M x 10k\n",
    "        scores = -norms\n",
    "\n",
    "        top_k = self.subset_sample(scores)\n",
    "        return top_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the convNet that we use to compute the features of the data examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, kernel_size=5, stride=1)\n",
    "        self.linear = nn.Linear(800, 500)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.linear(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hyperparameters.\n",
    "Here we say that we are going to using 100 queries per minibatch and 100 neighbors for the distance computation.\n",
    "The embedding size for each data sample is set to 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 9\n",
    "tau = 1.0\n",
    "NUM_TRAIN_QUERIES = 100\n",
    "NUM_TEST_QUERIES = 10\n",
    "NUM_TRAIN_NEIGHBORS = 100\n",
    "LEARNING_RATE = 10 **-3\n",
    "NUM_EPOCHS = 20\n",
    "EMBEDDING_SIZE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dknn_layer = SubsetsDKNN(k, tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sampler\n",
    "\n",
    "In theory we could use the entire dataset as the set of data points from which to choose the $k$ nearest neighbors for any query data point.\n",
    "This, however, is expensive so we instead use a random set of data points from which to select the $k$ nearest neighbors.\n",
    "\n",
    "In the following we define a data sampler class which samples a batch of query samples and a batch of neighbors from which to select the $k$ nearest neighbors for each query.\n",
    "The size of the query set and the neighbor set is fixed by the `NUM_TRAIN_QUERIES` and the `NUM_TRAIN_NEIGHBORS` hyperparameters defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 x,\n",
    "                 y,\n",
    "                 transform):\n",
    "\n",
    "        self.xy = TensorDataset(x, y)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.xy)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        x, y = self.xy[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "class DataSplit(object):\n",
    "    def __init__(self, dataset):\n",
    "        if dataset == 'mnist':\n",
    "            trva_real = datasets.MNIST(root='./data-mnist', download=True)\n",
    "            tr_real_ds, va_real_ds = random_split(trva_real, [55000, 5000])\n",
    "            xtr_real = trva_real.train_data[tr_real_ds.indices].view(\n",
    "                -1, 1, 28, 28)\n",
    "            ytr_real = trva_real.train_labels[tr_real_ds.indices]\n",
    "            xva_real = trva_real.train_data[va_real_ds.indices].view(\n",
    "                -1, 1, 28, 28)\n",
    "            yva_real = trva_real.train_labels[va_real_ds.indices]\n",
    "\n",
    "            trans = transforms.Compose(\n",
    "                [transforms.ToPILImage(), transforms.ToTensor()]\n",
    "            )\n",
    "\n",
    "            self.train_dataset = ClassicDataset(\n",
    "                x=xtr_real, y=ytr_real, transform=trans)\n",
    "            self.valid_dataset = ClassicDataset(\n",
    "                x=xva_real, y=yva_real, transform=trans)\n",
    "            self.test_dataset = datasets.MNIST(root='./data-mnist', train=False, transform=transforms.Compose([\n",
    "                transforms.ToTensor()\n",
    "            ]))\n",
    "            \n",
    "    def get_train_loader(self, batch_size, **kwargs):\n",
    "        train_loader = DataLoader(self.train_dataset,\n",
    "                                  batch_size=batch_size, num_workers=4, shuffle=True, **kwargs)\n",
    "        return train_loader\n",
    "\n",
    "    def get_valid_loader(self, batch_size, **kwargs):\n",
    "        valid_loader = DataLoader(self.valid_dataset,\n",
    "                                  batch_size=batch_size, shuffle=True, **kwargs)\n",
    "        return valid_loader\n",
    "\n",
    "    def get_test_loader(self, batch_size, **kwargs):\n",
    "        test_loader = DataLoader(self.test_dataset,\n",
    "                                 batch_size=batch_size, shuffle=False, **kwargs)\n",
    "        return test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "Now we compute the loss given the sampled subsets for the queries using the labels for the queries and neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dknn_loss(query, neighbors, query_label, neighbor_labels):\n",
    "    # query: batch_size x p\n",
    "    # neighbors: 10k x p\n",
    "    # query_labels: batch_size x [10] one-hot\n",
    "    # neighbor_labels: n x [10] one-hot\n",
    "\n",
    "    # num_samples x batch_size x n\n",
    "    start = time.time()\n",
    "    top_k_ness = dknn_layer(query, neighbors)\n",
    "    elapsed = time.time() - start\n",
    "    correct = (query_label.unsqueeze(1) *\n",
    "               neighbor_labels.unsqueeze(0)).sum(-1)  # batch_size x n\n",
    "    correct_in_top_k = (correct.unsqueeze(0) * top_k_ness).sum(-1)\n",
    "    loss = -correct_in_top_k\n",
    "\n",
    "    return loss, elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_phi = ConvNet().to(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    h_phi.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apervez/anaconda3/envs/pytorch1.9_2/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "/home/apervez/anaconda3/envs/pytorch1.9_2/lib/python3.7/site-packages/torchvision/datasets/mnist.py:62: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "/home/apervez/anaconda3/envs/pytorch1.9_2/lib/python3.7/site-packages/torchvision/datasets/mnist.py:52: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "split = DataSplit('mnist')\n",
    "\n",
    "batched_query_train = split.get_train_loader(NUM_TRAIN_QUERIES)\n",
    "batched_neighbor_train = split.get_train_loader(NUM_TRAIN_NEIGHBORS)\n",
    "\n",
    "# labels is a 1-dimensional tensor\n",
    "def one_hot(labels, l=10):\n",
    "    n = labels.shape[0]\n",
    "    labels = labels.unsqueeze(-1)\n",
    "    oh = torch.zeros(n, l, device='cuda').scatter_(1, labels, 1)\n",
    "    return oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "To train we sample a batch of queries and a batch of neighbors and compute the knn loss for each query and display the classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    timings = []\n",
    "    h_phi.train()\n",
    "    to_average = []\n",
    "    # train\n",
    "    for query, candidates in zip(batched_query_train, batched_neighbor_train):\n",
    "        optimizer.zero_grad()\n",
    "        cand_x, cand_y = candidates\n",
    "        query_x, query_y = query\n",
    "\n",
    "        cand_x = cand_x.to(device=gpu)\n",
    "        cand_y = cand_y.to(device=gpu)\n",
    "        query_x = query_x.to(device=gpu)\n",
    "        query_y = query_y.to(device=gpu)\n",
    "\n",
    "        neighbor_e = h_phi(cand_x).reshape(NUM_TRAIN_NEIGHBORS, EMBEDDING_SIZE)\n",
    "        query_e = h_phi(query_x).reshape(NUM_TRAIN_QUERIES, EMBEDDING_SIZE)\n",
    "\n",
    "        neighbor_y_oh = one_hot(cand_y).reshape(NUM_TRAIN_NEIGHBORS, 10)\n",
    "        query_y_oh = one_hot(query_y).reshape(NUM_TRAIN_QUERIES, 10)\n",
    "\n",
    "        losses, timing = dknn_loss(query_e, neighbor_e, query_y_oh, neighbor_y_oh)\n",
    "        timings.append(timing)\n",
    "        loss = losses.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        to_average.append((-loss).item() / k)\n",
    "\n",
    "    print('Avg. train correctness of top k:',\n",
    "          sum(to_average) / len(to_average))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing we can directly take the $k$ nearest neighbors and do not sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "\n",
    "def new_predict(query, neighbors, neighbor_labels):\n",
    "    '''\n",
    "    query: p\n",
    "    neighbors: n x p\n",
    "    neighbor_labels: n (int)\n",
    "    '''\n",
    "    diffs = (query.unsqueeze(1) - neighbors.unsqueeze(0))  # M x n x p\n",
    "    squared_diffs = diffs ** 2\n",
    "    norms = squared_diffs.sum(-1)  # M x n\n",
    "    indices = torch.argsort(norms, dim=-1)\n",
    "    labels = neighbor_labels.take(indices[:, :k])  # M x k\n",
    "    prediction = [majority(l.tolist()) for l in labels]\n",
    "    return torch.Tensor(prediction).to(device=gpu).long()\n",
    "\n",
    "\n",
    "def acc(query, neighbors, query_label, neighbor_labels):\n",
    "    prediction = new_predict(query, neighbors, neighbor_labels)\n",
    "    return (prediction == query_label).float().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_query_val = split.get_valid_loader(NUM_TEST_QUERIES)\n",
    "batched_query_test = split.get_test_loader(NUM_TEST_QUERIES)\n",
    "\n",
    "def test(epoch, val=False):\n",
    "    h_phi.eval()\n",
    "    global best_acc\n",
    "    with torch.no_grad():\n",
    "        embeddings = []\n",
    "        labels = []\n",
    "        for neighbor_x, neighbor_y in batched_neighbor_train:\n",
    "            neighbor_x = neighbor_x.to(device=gpu)\n",
    "            neighbor_y = neighbor_y.to(device=gpu)\n",
    "            embeddings.append(h_phi(neighbor_x))\n",
    "            labels.append(neighbor_y)\n",
    "        neighbors_e = torch.stack(embeddings).reshape(-1, EMBEDDING_SIZE)\n",
    "        labels = torch.stack(labels).reshape(-1)\n",
    "\n",
    "        results = []\n",
    "        for queries in batched_query_val if val else batched_query_test:\n",
    "            query_x, query_y = queries\n",
    "            query_x = query_x.to(device=gpu)\n",
    "            query_y = query_y.to(device=gpu)\n",
    "            query_e = h_phi(query_x)  # batch_size x embedding_size\n",
    "            results.append(acc(query_e, neighbors_e, query_y, labels))\n",
    "        total_acc = np.mean(np.array(results))\n",
    "\n",
    "    split = 'val' if val else 'test'\n",
    "    print('Avg. %s acc:' % split, total_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model. \n",
    "After 20 epochs of training we can get about 99.3% test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning epoch 1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apervez/anaconda3/envs/pytorch1.9_2/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. train correctness of top k: 0.798162693676322\n",
      "Beginning epoch 2: \n",
      "Avg. train correctness of top k: 0.9430294330673984\n",
      "Beginning epoch 3: \n",
      "Avg. train correctness of top k: 0.9600246477608737\n",
      "Beginning epoch 4: \n",
      "Avg. train correctness of top k: 0.9682177281620528\n",
      "Beginning epoch 5: \n",
      "Avg. train correctness of top k: 0.9722120181960283\n",
      "Beginning epoch 6: \n",
      "Avg. train correctness of top k: 0.9760807987174599\n",
      "Beginning epoch 7: \n",
      "Avg. train correctness of top k: 0.9786814417020248\n",
      "Beginning epoch 8: \n",
      "Avg. train correctness of top k: 0.9803707928127704\n",
      "Beginning epoch 9: \n",
      "Avg. train correctness of top k: 0.9808286046500168\n",
      "Beginning epoch 10: \n",
      "Avg. train correctness of top k: 0.9832377206436321\n",
      "Beginning epoch 11: \n",
      "Avg. train correctness of top k: 0.9841142041755441\n",
      "Beginning epoch 12: \n",
      "Avg. train correctness of top k: 0.9854074607232604\n",
      "Beginning epoch 13: \n",
      "Avg. train correctness of top k: 0.9857420248937129\n",
      "Beginning epoch 14: \n",
      "Avg. train correctness of top k: 0.9867219129234854\n",
      "Beginning epoch 15: \n",
      "Avg. train correctness of top k: 0.987558315161503\n",
      "Beginning epoch 16: \n",
      "Avg. train correctness of top k: 0.9883937643031893\n",
      "Beginning epoch 17: \n",
      "Avg. train correctness of top k: 0.988576520091355\n",
      "Beginning epoch 18: \n",
      "Avg. train correctness of top k: 0.9895413443054827\n",
      "Beginning epoch 19: \n",
      "Avg. train correctness of top k: 0.98968997531467\n",
      "Avg. test acc: 0.9918\n"
     ]
    }
   ],
   "source": [
    "for t in range(1, NUM_EPOCHS):\n",
    "    print('Beginning epoch %d: ' % t)\n",
    "    train(t)\n",
    "test(-1, val=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Reparameterizable Subset Sampling via Continuous Relaxations](https://arxiv.org/abs/1901.10517). [[Code](https://github.com/ermongroup/subsets)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

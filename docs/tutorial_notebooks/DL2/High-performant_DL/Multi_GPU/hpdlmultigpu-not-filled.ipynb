{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDL - Introduction to Multi GPU Programming\n",
    "\n",
    "**Filled notebook:** \n",
    "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/High-performant_DL/Multi_GPU/hpdlmultigpu.ipynb)    \n",
    "**Tutorial script files:** \n",
    "[![View scripts on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/saved_models/tree/main/DL2/High-performant_DL/Multi_GPU/scripts/)     \n",
    "**Recordings:** \n",
    "[![YouTube](https://img.shields.io/static/v1.svg?logo=youtube&label=YouTube&message=Watch&color=red)](https://www.youtube.com/watch?v=ZZFoCuiTbC4)    \n",
    "**Authors:** Samuele Papa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Using multiple GPUs is a central part in scaling models to large datasets and obtain state of the art performance. \n",
    "\n",
    "We have seen that, to control multiple GPUs, we need to understand the concepts of distributed computing. The core problem in distributed computing is the communication between nodes, which requires synchronization. Luckily, we are equipped with very limited communication tools, that minimize the chance that problems arise (the specifics are outside the scope of this course, to get more insight into the possible issues, look into [concurrent programming](https://en.wikipedia.org/wiki/Concurrent_computing), race conditions, deadlocks, resource starvation, semaphores and barriers, and the book Operating Systems Internals and Design Principles)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial content**. To better understand the primitives of communication in a distributed environment, we will begin by looking at some very basic exercises where simple operations are performed. Then, we will look at a more realistic scenario, the computation of the loss in a one-layer classifier (*more* realistic, but still *very* simple). Finally, we will learn how to run full-scale training on multiple GPUs and multiple nodes using PyTorch Lightning.\n",
    "\n",
    "#### Running the code\n",
    "The code in these cells is not meant to be run with this notebook. Instead, the files provided should be used in an environment where multiple GPUs are available. This step is not required (all the outputs and explanation of the code are available here), but highly encouraged, as getting familiar with these concepts, especially the more simple primitives, will help when more cryptic errors start appearing in big projects.\n",
    "\n",
    "Running the scripts can be done, for example, on the GPU partition of the LISA cluster ([General Knowledge on how to use the cluster](https://servicedesk.surfsara.nl/wiki/display/WIKI/SURFsara+Knowledge+Base)). After getting access using `ssh` (use WSL on Windows), we can setup the conda environment, by using the `module` package to load the correct anaconda version and then creating the environment based on the `environment.yml` file.\n",
    "\n",
    "To upload the code, the `rsync` command can be used (on single files, it is possible to do it on folders by adding the `-r` option):\n",
    "\n",
    "```\n",
    "rsync file account@lisa-gpu.surfsara.nl:~/file\n",
    "```\n",
    "\n",
    "Then, the Anaconda module can be loaded and the environment created using:\n",
    "```\n",
    "module load 2021\n",
    "module load Anaconda3/2021.05\n",
    "conda env create -f environment.yml\n",
    "```\n",
    "\n",
    "It will take some time to download the necessary packages.\n",
    "\n",
    "The main code to run is the following:\n",
    "```\n",
    "srun  -p gpu_shared -n 1 --ntasks-per-node 1 --gpus 2 --cpus-per-task 2 -t 1:00:00 --pty /bin/bash\n",
    "```\n",
    "\n",
    "where with `-p gpu_shared` we ask for the shared partition where there are GPUs available (other gpu partitions available are listed [here](https://servicedesk.surfsara.nl/wiki/display/WIKI/Lisa+usage+and+accounting)), then, we specify that we will be running only 1 task in this node, we want 2 GPUs and we use 2 CPUs as well, for 1 hour. The run consists of executing the command `/bin/bash` which starts a bash console on the node that we have been assigned. This allows for input of the necessary commands. \n",
    "\n",
    "Once inside, we can activate the correct anaconda environment and start running the scripts. We need to make sure that both GPUs are exposed to the script, with the following syntax:\n",
    "\n",
    "```\n",
    "python my_script.py\n",
    "```\n",
    "\n",
    "For these examples we will make use of the straightforward interface provided by [PyTorch](https://pytorch.org/docs/stable/distributed.html), a good summary is available at the documentation page, where all the details of the functions are shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some useful utilities\n",
    "\n",
    "The following code will help in the running of the experiments, with some plotting functions and setup of the distributed environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def rank_print(text: str):\n",
    "    \"\"\"\n",
    "    Prints a statement with an indication of what node rank is sending it\n",
    "    \"\"\"\n",
    "    rank = dist.get_rank()\n",
    "    # Keep the print statement as a one-liner to guarantee that\n",
    "    # one single process prints all the lines\n",
    "    print(f\"Rank: {rank}, {text}.\")\n",
    "\n",
    "\n",
    "def disk(\n",
    "    matrix: torch.Tensor,\n",
    "    center: tuple[int, int] = (1, 1),\n",
    "    radius: int = 1,\n",
    "    value: float = 1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Places a disk with a certain radius and center in a matrix. The value given to the disk must be defined.\n",
    "    Something like this:\n",
    "    0 0 0 0 0\n",
    "    0 0 1 0 0\n",
    "    0 1 1 1 0\n",
    "    0 0 1 0 0\n",
    "    0 0 0 0 0\n",
    "\n",
    "    Arguments:\n",
    "     - matrix: the matrix where to place the shape.\n",
    "     - center: a tuple indicating the center of the disk\n",
    "     - radius: the radius of the disk in pixels\n",
    "     - value: the value to write where the disk is placed\n",
    "    \"\"\"\n",
    "    device = matrix.get_device()\n",
    "    shape = matrix.shape\n",
    "\n",
    "    # genereate the grid for the support points\n",
    "    # centered at the position indicated by position\n",
    "    grid = [slice(-x0, dim - x0) for x0, dim in zip(center, shape)]\n",
    "    x_coords, y_coords = np.mgrid[grid]\n",
    "    mask = torch.tensor(\n",
    "        ((x_coords / radius) ** 2 + (y_coords / radius) ** 2 <= 1), device=device\n",
    "    )\n",
    "    matrix = matrix * (~mask) + mask * value\n",
    "\n",
    "    return matrix, mask\n",
    "\n",
    "\n",
    "def square(\n",
    "    matrix: torch.tensor,\n",
    "    topleft: tuple[int, int] = (0, 0),\n",
    "    length: int = 1,\n",
    "    value: float = 1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Places a square starting from the given top-left position and having given side length.\n",
    "    The value given to the disk must be defined.\n",
    "    Something like this:\n",
    "    0 0 0 0 0\n",
    "    0 1 1 1 0\n",
    "    0 1 1 1 0\n",
    "    0 1 1 1 0\n",
    "    0 0 0 0 0\n",
    "\n",
    "    Arguments:\n",
    "     - matrix: the matrix where to place the shape.\n",
    "     - topleft: a tuple indicating the top-left-most vertex of the square\n",
    "     - length: the side length of the square\n",
    "     - value: the value to write where the square is placed\n",
    "    \"\"\"\n",
    "    device = matrix.get_device()\n",
    "    shape = matrix.shape\n",
    "    grid = [slice(-x0, dim - x0) for x0, dim in zip(topleft, shape)]\n",
    "    x_coords, y_coords = np.mgrid[grid]\n",
    "    mask = torch.tensor(\n",
    "        (\n",
    "            (x_coords <= length)\n",
    "            & (x_coords >= 0)\n",
    "            & (y_coords >= 0)\n",
    "            & (y_coords <= length)\n",
    "        ),\n",
    "        device=device,\n",
    "    )\n",
    "    matrix = matrix * (~mask) + mask * value\n",
    "\n",
    "    return matrix, mask\n",
    "\n",
    "\n",
    "def plot_matrix(\n",
    "    matrix: torch.Tensor,\n",
    "    rank: int,\n",
    "    title: str = \"Matrix\",\n",
    "    name: str = \"image\",\n",
    "    folder: Optional[str] = None,\n",
    "    storefig: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Helper function to plot the images more easily. Can store them or visualize them right away.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.imshow(matrix.cpu(), cmap=\"tab20\", vmin=0, vmax=19)\n",
    "    plt.axis(\"off\")\n",
    "    if folder:\n",
    "        folder = Path(folder)\n",
    "        folder.mkdir(exist_ok=True, parents=True)\n",
    "    else:\n",
    "        folder = Path(\".\")\n",
    "\n",
    "    if storefig:\n",
    "        plt.savefig(folder / Path(f\"rank_{rank}_{name}.png\"))\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When starting the distributed environment, we need to decide a backend between `gloo`, `nccl` and `mpi`. The support for these libraries needs to be already available. The `nccl` backend should be already available from a GPU installation of PyTorch (CUDA Toolkit is required). On a Windows environment, only `gloo` works, but we will be running these scripts on a Unix environment.\n",
    "\n",
    "The second fundamental aspect is how the information is shared between nodes. The method we choose is through a shared file, that is accessible from all the GPUs. It is important to remember that access to this file should be quick for all nodes, so on LISA we will put it in the `scratch` folder.\n",
    "\n",
    "The other two parameters are the `rank` and `world_size`. The rank refers to the identifier for the current device, while the world size is the number of devices available for computation.\n",
    "\n",
    "When setting up the distributed environment, the correct GPU device should be selected. For simplicity, we select the GPU that has ID corresponding to the rank, but this is not necessary. \n",
    "\n",
    "Computation nodes could reside in different nodes, when this happens, using a shared fil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def setup_distrib(\n",
    "    rank: int,\n",
    "    world_size: int,\n",
    "    init_method: str = \"file:///scratch/sharedfile\",\n",
    "    backend: str = \"nccl\",\n",
    "):\n",
    "    # select the correct device for this process\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "    # initialize the processing group\n",
    "    torch.distributed.init_process_group(\n",
    "        backend=backend, world_size=world_size, init_method=init_method, rank=rank\n",
    "    )\n",
    "\n",
    "    # return the current device\n",
    "    return torch.device(f\"cuda:{rank}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a model on multiple GPUs is a clear example that we will keep in mind throughout this tutorial to contextualize how to make use of the available primitives of communication in distributed computing.\n",
    "\n",
    "Initially, we want to give the current weights of the model to every GPU that we are using. To do so, we will **broadcast** the necessary tensors.\n",
    "\n",
    "Then, each GPU will collect a subset of the full batch, lets say only 64 out of 256 samples, from memory and perform a forward pass of the model. At the end, we need to compute the loss over the entire batch of 256 samples, but no GPU can fit all of these. Here, the **reduction** primitive comes to the rescue. The tensors that reside in different GPUs are collected and an operation is performed that will *reduce* the tensors to a single one. This allows for the result of the operation to still fit in memory. We may want to keep this result in a single GPU (using **reduce**) or send it to all of them (using **all_reduce**).\n",
    "\n",
    "The operations that we can perform are determined by the backend that we are currently using. When using `nccl`, the list of available operations is the following:\n",
    " - `SUM`\n",
    " - `AVG` (only version 2.10 or higher)\n",
    " - `PRODUCT`\n",
    " - `MIN`\n",
    " - `MAX`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communication Primitives\n",
    "**Try first on your own**\n",
    "\n",
    "If you want to first try on your own, you will see the prompts of the small demos created and the expected results. Create separate files for each demo and then run it as instructed previously. Use the official PyTorch documentation to do this and the following code to spawn multiple processes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from utils import setup_distrib, disk, square, rank_print, plot_matrix\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch\n",
    "\n",
    "def main_process(rank:int, world_size:int=2):\n",
    "    # Here your code ...\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mp.spawn(main_process, nprocs=2, args=())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a shape, first initialize an empty tensor, and then place the shape using the function. The function outputs both the updated tensor with the shape added, and a mask indicating where the shape was placed. For all our applications we will just need the new tensor, not the mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "image = torch.ones((11,11), device=current_device)\n",
    "# Place a disk\n",
    "disk(matrix=image, \n",
    "     center=(4, 5), \n",
    "     radius=2, \n",
    "     value=7)[0]\n",
    "\n",
    "# Place a square\n",
    "square(matrix=image, \n",
    "       topleft=(1, 2), \n",
    "       length=2, \n",
    "       value=3)[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Reduce\n",
    "\n",
    "As we can see from the illustration, the **all reduce** primitive performs an operation between the tensors present in each GPU and replaces them with the result of the operation. \n",
    "\n",
    "**Put a disk in the rank 0 device and a square in rank 1 device, then perform the all reduce operation and look at the results.**\n",
    "\n",
    "<center width=\"100%\"><img style=\"margin:0 auto\" src=\"assets/allreduce.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this operation is illustrated in the figure below. The operation is performed between the tensors stored in the different devices and the result is spread across all devices.\n",
    "\n",
    "<center width=\"100%\"><img style=\"margin:0 auto\" src=\"assets/slide_all_reduce.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce\n",
    "\n",
    "As we can see from the illustration, the **reduce** primitive performs an operation between the tensors present in each GPU and sends the result only to the root rank. In Pytorch, we can define the destination rank. \n",
    "\n",
    "**Again, put a disk in the rank 0 device and a square in the rank 1 device, this time run the reduce operation with, as destination rank the device you prefer.**\n",
    "\n",
    "<center width=\"100%\"><img style=\"margin:0 auto\" src=\"assets/reduce.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are shown below. We can see how only the rank 0, the one we selected, has the result of the operation. This helps in reducing the processing time, if the operation is executed in an asynchronous way, all other GPUs can keep processing while the root one is receiving the result.\n",
    "\n",
    "<center width=\"100%\"><img style=\"margin:0 auto\" src=\"assets/slide_reduce.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast\n",
    "\n",
    "The **broadcast** operation is fundamental, as it allows to send (broadcast) data from one GPU to all others in the network. \n",
    "\n",
    "**Create a shape (we chose a disk) in the rank 0 device and then broadcast it to all others.**\n",
    "\n",
    "<center width=\"100%\"><img style=\"margin:0 auto\" src=\"assets/broadcast.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this illustration we see how, the rank 1 GPU gets the correct image after the broadcast is performed.\n",
    "\n",
    "<center width=\"100%\"><img style=\"margin:0 auto\" src=\"assets/slide_broadcast.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Gather\n",
    "\n",
    "The **all gather** operation allows for all GPUs to have access to all the data processed by the others. This can be especially useful when different operations need to be performed by each GPU, after a common operation has been performed on each subset of the data. It is important to note that the entirety of the data needs to fit in a single GPU, so here the bottleneck won't be the memory, instead, it will be the processing speed. \n",
    "\n",
    "**Place a disk in the rank 0 device and a square in the rank 1 device, then create a list of empty tensors and use it as target for the all gather operation.**\n",
    "\n",
    "<center width=\"100%\"><img style=\"margin:0 auto\" src=\"assets/allgather.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the result of the **all_gather**. All GPUs now have access to the data that was initially only present in some of them.\n",
    "\n",
    "<center width=\"100%\"><img style=\"margin:0 auto\" src=\"assets/slide_all_gather.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Scatter\n",
    "\n",
    "With **reduce scatter** we can perform an operation on just a subset of the whole data and have each GPU have the partial results.\n",
    "\n",
    "**Create a list of shapes in the rank 0 and rank 1 devices (make them different to see interesting results), then run the reduce scatter operation.**\n",
    "\n",
    "<center width=\"100%\"><img style=\"margin:0 auto\" src=\"assets/reducescatter.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this figure we see that only one image is available at the end, allowing for the operation to be performed across GPUs while keeping the overall final memory footprint low.\n",
    "<center width=\"100%\"><img style=\"margin:0 auto\" src=\"assets/slide_reduce_scatter.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "An interesting thing to test, if you have access to a multi-GPU environment, is what are the physical limits of the system, and if the processing speed is the same with any number of tensors being loaded into the GPU. Is it more efficient to use a multiple of the number of cores that are processing the data in the GPU, or is the difference in performance negligible? You can investigate these topics through experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more comprehensive example\n",
    "\n",
    "We will now look at a more realistic scenario (code in `single_layer.py`), the overall process is shown in the figure below.\n",
    "\n",
    "<center width=\"100%\"><img style=\"margin:0 auto\" src=\"assets/example_explanation.png\" /></center>\n",
    "\n",
    "The first thing we do is to spawn the two processes. \n",
    "In each, we begin by initializing the distributed processing environment.\n",
    "\n",
    "Then, the datasets needs to be downloaded. Here, I assume that it has not been downloaded yet, and I only let the GPU in rank 0 perform this operation. This avoids having two processes writing in the same file. In order to have the other process wait for the first one to download, a **barrier** is used. The working principle is very simple, when a barrier is reached in the code, the process waits for all other processes to also reach that point in the code. Here we see how this can be a very useful construct in parallel computing, all processes require the dataset to be downloaded before proceeding, so one of them starts the download, and all wait until it's done.\n",
    "\n",
    "Then we initialize the weights, only in the rank 0 GPU, and **broadcast** them to all other GPUs. This broadcast operation is performed asynchronously, to allow for the rank 0 GPU to start loading images before the rank 1 has received the weights. This operation is akin to what DataParallel does, which is slowing the processing of the other GPUs down, waiting to receive the weights from the root GPU.\n",
    "\n",
    "<center width=\"100%\"><img style=\"margin:0 auto\" src=\"assets/example_broadcast.png\" /></center>\n",
    "\n",
    "Each GPU will then load the images from disk, perform a product to find the activations of the next layer and calculate a softmax to get class-belonging probabilities. \n",
    "\n",
    "Finally, the loss is computed by summing over the dimensions and a **reduction** with sum is performed to compute the overall loss over the entire batch.\n",
    "\n",
    "<center width=\"100%\"><img style=\"margin:0 auto\" src=\"assets/example_reduction.png\" /></center>\n",
    "\n",
    "**You can now try and write this code for yourself.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from utils import rank_print\n",
    "\n",
    "DSET_FOLDER = \"/scratch/\"\n",
    "\n",
    "def main_process(rank, world_size=2):\n",
    "    print(f\"Process for rank: {rank} has been spawned\")\n",
    "\n",
    "    # Setup the distributed processing\n",
    "    device = setup_distrib(rank, world_size)\n",
    "\n",
    "    # Download dataset only in the first device\n",
    "    if rank == 0:\n",
    "        ...\n",
    "    \n",
    "    # Make sure download has finished and other devices cannot keep going\n",
    "    ...\n",
    "    \n",
    "    # Load the dataset\n",
    "    dset = ...\n",
    "\n",
    "    input_size = 3 * 32 * 32  # [channel size, height, width]\n",
    "    per_gpu_batch_size = 128\n",
    "    num_classes = 10\n",
    "    # Initialize weigths as a random tensor in the first device, and a tensor of\n",
    "    #  zeros in the other devices\n",
    "    if dist.get_rank() == 0:\n",
    "        weights = torch.rand((input_size, num_classes), device=device)\n",
    "    else:\n",
    "        weights = torch.zeros((input_size, num_classes), device=device)\n",
    "\n",
    "    # Distribute weights to all GPUs\n",
    "    ...\n",
    "    rank_print(f\"Weights received.\")\n",
    "\n",
    "    # Flattened images\n",
    "    cur_input = torch.zeros((per_gpu_batch_size, input_size), device=device)\n",
    "    # One-Hot encoded target\n",
    "    cur_target = torch.zeros((per_gpu_batch_size, num_classes), device=device)\n",
    "    for i in range(per_gpu_batch_size):\n",
    "        rank_print(f\"Loading image {i+world_size*rank} into GPU...\")\n",
    "        image, target = dset[i + world_size * rank]\n",
    "        cur_input[i] = transforms.ToTensor()(image).flatten()\n",
    "        cur_target[i, target] = 1.0\n",
    "\n",
    "    # Compute the linear part of the layer with a matrix multiplication\n",
    "    output = ...\n",
    "    rank_print(f\"\\nComputed output: {output}, Size: {output.size()}.\")\n",
    "\n",
    "    # Define the activation function of the output layer\n",
    "    logsoftm = ...\n",
    "\n",
    "    # Apply activation function to output layer\n",
    "    output = logsoftm(output)\n",
    "    rank_print(f\"\\nLog-Softmaxed output: {output}, Size: {output.size()}.\")\n",
    "\n",
    "    loss = output.sum(dim=1).mean()\n",
    "    rank_print(f\"Loss: {loss}, Size: {loss.size()}\")\n",
    "\n",
    "    # Compute a reduction with SUM operation\n",
    "    ...\n",
    "    rank_print(f\"Final Loss: {loss/world_size}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mp.spawn(main_process, nprocs=2, args=())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch  Lightning\n",
    "\n",
    "When you have your Pytorch Lightning module defined, scaling to multiple GPUs and multi nodes is very straightforward (more details [here](https://pytorch-lightning.readthedocs.io/en/stable/advanced/multi_gpu.html)):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "trainer = Trainer(gpus=8, strategy=\"ddp\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, seems impossible, but it's real. In most cases this is all you need to run multi GPU training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have seen how the basic collective primitives for communication work in a multi GPU environment. The reduction and broadcast operations are maybe the most important ones, allowing for delivery of data to all nodes and to perform mathematical operations on the data present in all the nodes. \n",
    "\n",
    "We have seen how we can use these collectives to perform a calculation of the loss of a neural network, but the same can be extended to any type of parallelizable computation.\n",
    "\n",
    "Finally, we saw how simple it is to set a PyTorch Lightning training to use multiple GPUs.\n",
    "\n",
    "### References\n",
    "\n",
    "Pytorch Documentation on Distributed Communication. https://pytorch.org/docs/stable/distributed.html\n",
    "\n",
    "NVIDIA NCCL Developer Guide. https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/overview.html\n",
    "\n",
    "PyTorch Lightning Multi-GPU Training. https://pytorch-lightning.readthedocs.io/en/stable/advanced/multi_gpu.html\n",
    "\n",
    "Concurrent Programming and Operating Systems. Stallings, William. Operating Systems : Internals and Design Principles. Upper Saddle River, N.J. :Prentice Hall, 2001."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

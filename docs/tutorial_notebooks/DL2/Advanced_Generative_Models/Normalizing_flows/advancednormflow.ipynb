{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGM - Advanced Topics in Normalizing Flows - 1x1 convolution\n",
    "\n",
    "**Filled notebook:** \n",
    "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/Advanced_Generative_Models/Normalizing_flows/advancednormflow.ipynb)\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/Advanced_Generative_Models/Normalizing_flows/advancednormflow.ipynb)    \n",
    "**Authors:**\n",
    "Cyril Hsu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The [Glow](https://arxiv.org/abs/1807.03039), a flow-based generative model extends the previous invertible generative models, [NICE](https://arxiv.org/abs/1410.8516) and [RealNVP](https://arxiv.org/abs/1605.08803), and simplifies the architecture by replacing the reverse permutation operation on the channel ordering with **Invertible 1x1 Convolutions**. Glow is famous for being the one of the first flow-based models that works on high resolution images and enables manipulation in latent space. Let's have a look at the interactive [demonstration](https://openai.com/blog/glow/) from OpenAI.\n",
    "\n",
    "<center width=\"100%\"><img src=\"assets/face_demo.gif\" width=\"350px\" style=\"padding: 20px; margin:0 auto\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glow consists of a series of steps of flow. Each step of flow comprises **Actnorm** followed by an **Invertible 1×1 Convolution**, and finally a **Coupling Layer**.\n",
    "<center width=\"100%\"><img src=\"assets/glow_bb.png\" width=\"350px\" style=\"padding: 20px; margin:0 auto\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actnorm** performs an affine transformation with a scale and bias parameter per channel, similar to that of batch normalization, but works on mini-batch size 1. The statistics (mean and std), however, are only calculated once to initialize the scale and bias parameters.\n",
    "\n",
    "**Invertible 1×1 Convolution** with equal number of input and output channels is a generalization of any permutation of the channel ordering. Recall the operation between layers of the RealNVP flow, the ordering of channels is switched so that all the data dimensions have a chance to be mixed. 1x1 convolution is proposed to replace this fixed permutation with a learned invertible operation.\n",
    "\n",
    "**Coupling Layer** is a powerful reversible transformation where the forward function, the reverse function and the logdeterminant are computationally efficient. The design is the same as in RealNVP.\n",
    "\n",
    "<center width=\"100%\"><img src=\"assets/glow_comp.png\" width=\"800px\" style=\"padding: 20px; margin:0 auto\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will be focusing on the implementation of invertible 1x1 convolution layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invertible 1x1 convolution\n",
    "\n",
    "Given an input of shape $H\\times W\\times C$ applied with a 1x1 convolution with $C$ filters, meaning the output tensor shape is also going to be $H\\times W\\times C$. Thus, each layer has a set of weights $W$ with $C\\times C$ values.\n",
    "The forward operation acts just like a typical convolution, while the inverse operation can be computed by simply applying a convolution with $W^{-1}$ weights.\n",
    "<center width=\"100%\"><img src=\"assets/1x1.png\" width=\"500px\" style=\"padding: 20px; margin:0 auto\"></center>\n",
    "\n",
    "Enough descriptions! Now let's take a look at the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class InvConv2d(nn.Module):\n",
    "    def __init__(self, in_channel):\n",
    "        super().__init__()\n",
    "\n",
    "        weight = torch.randn(in_channel, in_channel)\n",
    "        # use the Q matrix from QR decomposition as the initial weight to make sure it's invertible\n",
    "        q, _ = torch.qr(weight)\n",
    "        weight = q.unsqueeze(2).unsqueeze(3)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "\n",
    "    def forward(self, input, logdet, reverse=False):\n",
    "        _, _, height, width = input.shape\n",
    "        \n",
    "        # You can also use torch.slogdet(self.weight)[1] to summarize the operations below\\n\",\n",
    "        dlogdet = (\n",
    "            height * width * torch.log(torch.abs(torch.det(self.weight.squeeze())))\n",
    "        )\n",
    "\n",
    "        if not reverse:\n",
    "            out = F.conv2d(input, self.weight)\n",
    "            logdet = logdet + dlogdet\n",
    "\n",
    "        else:\n",
    "            out = F.conv2d(input, self.weight.squeeze().inverse().unsqueeze(2).unsqueeze(3))\n",
    "            logdet = logdet - dlogdet\n",
    "\n",
    "        return out, logdet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that to calculate the determinant of $W$ could be computationally expensive, thus there's also an [implementation](https://github.com/rosinality/glow-pytorch/blob/master/model.py#L88) which utilizes LU decomposition to speed up, as suggested in the Glow paper.\n",
    "\n",
    "The idea is to parameterizing $W$ directly in its LU decomposition:\n",
    "\n",
    "$$\n",
    "W = PL(U + \\text{diag}(s)),\n",
    "$$\n",
    "\n",
    "where $P$ is a permutation matrix, $L$ is a lower triangular matrix with ones on the diagonal, $U$ is an upper triangular matrix with zeros on the diagonal, and $s$ is a vector.\n",
    "\n",
    "The log-determinant is then simply:\n",
    "\n",
    "$$\n",
    "\\log | \\det(W)| = \\sum \\left(\\log |s|\\right)\n",
    "$$\n",
    "\n",
    "Please check out the link above for the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A small pitfall\n",
    "As you might notice, there's an inverse operation for the weight $W$ involved when the **1x1 convolution** is forwarding reversely. As a result, an error can occur when the weight $W$ is not invertible, even though it seldom happens.\n",
    "\n",
    "To our best knowledge, there's no elegant solution to address this, but an easy way to workaround: If this happens unfortunately during the training, one can try to restart from the recent checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A complete flow block\n",
    "\n",
    "Now we have the **Invertible 1x1 Convolution**. Together with the aforementioned **Actnorm** and **Coupling Layer**, we are ready to try out the power of the Glow by plugging the block into the model we had in the NFs tutorial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActNorm(nn.Module):\n",
    "    def __init__(self, in_channel):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loc = nn.Parameter(torch.zeros(1, in_channel, 1, 1))\n",
    "        self.log_scale = nn.Parameter(torch.zeros(1, in_channel, 1, 1))\n",
    "        self.register_buffer(\"initialized\", torch.tensor(0, dtype=torch.uint8))\n",
    "\n",
    "    def initialize(self, input):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            flatten = input.permute(1, 0, 2, 3).contiguous().view(input.shape[1], -1)\n",
    "            mean = (\n",
    "                flatten.mean(1)\n",
    "                .unsqueeze(1)\n",
    "                .unsqueeze(2)\n",
    "                .unsqueeze(3)\n",
    "                .permute(1, 0, 2, 3)\n",
    "            )\n",
    "            std = (\n",
    "                flatten.std(1)\n",
    "                .unsqueeze(1)\n",
    "                .unsqueeze(2)\n",
    "                .unsqueeze(3)\n",
    "                .permute(1, 0, 2, 3)\n",
    "            )\n",
    "\n",
    "            self.loc.data.copy_(-mean)\n",
    "            self.log_scale.data.copy_(-std.clamp_(min=1e-6).log())\n",
    "\n",
    "    def forward(self, input, logdet, reverse=False):\n",
    "        _, _, height, width = input.shape\n",
    "\n",
    "        if self.initialized.item() == 0:\n",
    "            self.initialize(input)\n",
    "            self.initialized.fill_(1)\n",
    "\n",
    "        dlogdet = height * width * torch.sum(self.log_scale)\n",
    "\n",
    "        if not reverse:\n",
    "            logdet += dlogdet\n",
    "            return self.log_scale.exp() * (input + self.loc), logdet\n",
    "\n",
    "        else:\n",
    "            dlogdet *= -1\n",
    "            logdet += dlogdet\n",
    "            return input / self.log_scale.exp() - self.loc, logdet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample code for **Actnorm** is provided above.\n",
    "\n",
    "As for **Coupling Layer**, please refer to the one in the NFs tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've learned an advanced flow-based layer from the Glow model, an **Invertible 1x1 convolution**, which is adapted from the typical 1x1 convolution layers.\n",
    "\n",
    "### References\n",
    "\n",
    "* [Glow: Generative Flow with Invertible 1x1 Convolutions](https://arxiv.org/abs/1807.03039)\n",
    "* [Glow: Better Reversible Generative Models](https://openai.com/blog/glow/)\n",
    "* https://github.com/rosinality/glow-pytorch\n",
    "* [Materials from NTU Speech Lab](https://reurl.cc/9O8bka)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

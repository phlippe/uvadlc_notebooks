{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKCc72_G_pBL"
      },
      "source": [
        "# Tutorial 2: Comparing to Non-Bayesian Methods for Uncertainty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Filled notebook:** \n",
        "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/Bayesian_Neural_Networks/Student_DLII_BNN_2_2.ipynb)\n",
        "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1XnRx53b-7MUgXmdrJn6h5y__tR2XMNvu?usp=sharing)  \n",
        "**Authors:**\n",
        "Ilze Amanda Auzina, Leonard Bereska and Eric Nalisnick"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-bJ1LbSpXy2"
      },
      "source": [
        "In this tutorial we will investigate benefits of Bayesian Neural Networks (BNNs) over point estimate Neural Networks (NNs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nApengk56uHi"
      },
      "source": [
        "Import standard libraries and setting random seeds for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQ3dCabzEHko"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm.auto import trange\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U52YDHOLOPr_"
      },
      "source": [
        "### Simulate Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2aP1KBC6x2V"
      },
      "source": [
        "Let's simulate a wiggly line and draw observations in separated regions..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rxrRKRUKWBP"
      },
      "outputs": [],
      "source": [
        "def get_simple_data_train():\n",
        "    x = np.linspace(-.2, 0.2, 500)\n",
        "    x = np.hstack([x, np.linspace(.6, 1, 500)])\n",
        "    eps = 0.02 * np.random.randn(x.shape[0])\n",
        "    y = x + 0.3 * np.sin(2 * np.pi * (x + eps)) + 0.3 * np.sin(4 * np.pi * (x + eps)) + eps\n",
        "    x_train = torch.from_numpy(x).float()[:, None]\n",
        "    y_train = torch.from_numpy(y).float()\n",
        "    return x_train, y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzMSFOd6KWBP"
      },
      "outputs": [],
      "source": [
        "def plot_generic(add_to_plot=None):\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    plt.xlim([-.5, 1.5])\n",
        "    plt.ylim([-1.5, 2.5])\n",
        "    plt.xlabel(\"X\", fontsize=30)\n",
        "    plt.ylabel(\"Y\", fontsize=30)\n",
        "\n",
        "    x_train, y_train = get_simple_data_train()\n",
        "    \n",
        "    x_true = np.linspace(-.5, 1.5, 1000)\n",
        "    y_true = x_true + 0.3 * np.sin(2 * np.pi * x_true) + 0.3 * np.sin(4 * np.pi * x_true)\n",
        "    \n",
        "    ax.plot(x_train, y_train, 'ko', markersize=4, label=\"observations\")\n",
        "    ax.plot(x_true, y_true, 'b-', linewidth=3, label=\"true function\")\n",
        "    if add_to_plot is not None:\n",
        "        add_to_plot(ax)\n",
        "\n",
        "    plt.legend(loc=4, fontsize=15, frameon=False)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKA0taHjKWBQ"
      },
      "outputs": [],
      "source": [
        "plot_generic()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAdi31vnroy_"
      },
      "source": [
        "As you can see, we have the true function in blue. The observations are observable in two regions of the function and there is some noise in their measurement. We will use this simple data to showcase the differences between BNNs and deterministic NNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLBktPK5b4K2"
      },
      "source": [
        "### Define non-Bayesian Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxvOkEoLqwfW"
      },
      "source": [
        "First let's create our point estimate neural network, in other words a standard fully connected MLP. We will define the number of hidden layers dynamically so we can reuse the same class for different depths.  We will also add a *dropout* flag, this will allow us to easily use the same architecture for our BNN. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8U6HQN__KWBR"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim=1, output_dim=1, hidden_dim=10, n_hidden_layers=1, use_dropout=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.use_dropout = use_dropout\n",
        "        if use_dropout:\n",
        "            self.dropout = nn.Dropout(p=0.5)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "        # dynamically define architecture\n",
        "        self.layer_sizes = [input_dim] + n_hidden_layers * [hidden_dim] + [output_dim]\n",
        "        layer_list = [nn.Linear(self.layer_sizes[idx - 1], self.layer_sizes[idx]) for idx in\n",
        "                      range(1, len(self.layer_sizes))]\n",
        "        self.layers = nn.ModuleList(layer_list)\n",
        "\n",
        "    def forward(self, input):\n",
        "        hidden = self.activation(self.layers[0](input))\n",
        "        for layer in self.layers[1:-1]:\n",
        "            hidden_temp = self.activation(layer(hidden))\n",
        "\n",
        "            if self.use_dropout:\n",
        "                hidden_temp = self.dropout(hidden_temp)\n",
        "\n",
        "            hidden = hidden_temp + hidden  # residual connection\n",
        "\n",
        "        output_mean = self.layers[-1](hidden).squeeze()\n",
        "        return output_mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YNYTHT2ldSW"
      },
      "source": [
        "### Train one deterministic NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8vF6e96skkO"
      },
      "source": [
        "**Training**\n",
        "\n",
        "Now let's train our MLP with the training data we generated above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4GjzP7zKWBS"
      },
      "outputs": [],
      "source": [
        "def train(net, train_data):\n",
        "    x_train, y_train = train_data\n",
        "    optimizer = torch.optim.Adam(params=net.parameters(), lr=1e-3)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    progress_bar = trange(3000)\n",
        "    for _ in progress_bar:\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(y_train, net(x_train))\n",
        "        progress_bar.set_postfix(loss=f'{loss / x_train.shape[0]:.3f}')\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hl2wgi_SKWBS"
      },
      "outputs": [],
      "source": [
        "train_data = get_simple_data_train()\n",
        "x_test = torch.linspace(-.5, 1.5, 3000)[:, None]  # test over the whole range\n",
        "\n",
        "net = MLP(hidden_dim=30, n_hidden_layers=2)\n",
        "net = train(net, train_data)\n",
        "y_preds = net(x_test).clone().detach().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijNbCnEAs45Q"
      },
      "source": [
        "**Evaluate**\n",
        "\n",
        "Let's investigate how our deterministic MLP generalizes over the entire domain of our input variable $x$ (the model was only trained on the observations, now we will also pass in data outside this region)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09BvSRf4KWBT"
      },
      "outputs": [],
      "source": [
        "def plot_predictions(x_test, y_preds):\n",
        "    def add_predictions(ax):\n",
        "        ax.plot(x_test, y_preds, 'r-', linewidth=3, label='neural net prediction')\n",
        "\n",
        "    plot_generic(add_predictions)\n",
        "plot_predictions(x_test, y_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alkSSxGDv6K2"
      },
      "source": [
        "We can see that our deterministic MLP (red line) has correctly learned the data distribution in the training regions, however, as the model has not learned the underlying sinusoidal wave function, it's predictions outside the training region are inaccurate. As our MLP is a point estimate NN we have no measure confidence in the predictions outside the training region. In the upcoming sections let's see how this compares to BNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpeP23TD9vIf"
      },
      "source": [
        "### Deep Ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMoU4HgQ1Tmz"
      },
      "source": [
        "Deep ensembles were first introduced by [Lakshminarayanan et al. (2017)](https://arxiv.org/abs/1612.01474). As the name implies multiple point estimate NN are trained, *an ensemble*, and the final prediction is computed as an average across the models. From a Bayesian perspective the different point estimates correspond to modes of a Bayesian posterior. This can be interpreted as approximating the posterior with a distribution parametrized as multiple Dirac deltas:\n",
        "\n",
        "$$\n",
        "q_{\\phi}(\\theta | D) = \\sum_{\\theta_{i} ∈ ϕ} \\alpha_{\\theta_{i}} δ_{\\theta_{i}}(\\theta)\n",
        "$$\n",
        "where $\\alpha_{\\theta_{i}}$ are positive constants such that their sum is equal to one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB6fW21Nlvax"
      },
      "source": [
        "**Training**\n",
        "\n",
        "We will reuse the MLP architecture introduced before, simply now we will train an ensemble of such models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpWs-KMzKWBV"
      },
      "outputs": [],
      "source": [
        "ensemble_size = 5\n",
        "ensemble = [MLP(hidden_dim=30, n_hidden_layers=2) for _ in range(ensemble_size)]\n",
        "for net in ensemble:\n",
        "    train(net, train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uapMWFHu5VUq"
      },
      "source": [
        "**Evaluate**\n",
        "\n",
        "Same as before, let's investigate how our Deep Ensemble performs on the entire data domain of our input variable $x$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MODFQq5NKWBV"
      },
      "outputs": [],
      "source": [
        "y_preds = [np.array(net(x_test).clone().detach().numpy()) for net in ensemble]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqBYQflKlyEF"
      },
      "source": [
        "Plot each ensemble member's predictive function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNXC0YyZKWBV"
      },
      "outputs": [],
      "source": [
        "def plot_multiple_predictions(x_test, y_preds):\n",
        "    def add_multiple_predictions(ax):\n",
        "        for idx in range(len(y_preds)):\n",
        "            ax.plot(x_test, y_preds[idx], '-', linewidth=3)\n",
        "\n",
        "    plot_generic(add_multiple_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHcEBaG0KWBW"
      },
      "outputs": [],
      "source": [
        "plot_multiple_predictions(x_test, y_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBFTVQDI54Uv"
      },
      "source": [
        "In this plot the benefit of an ensemble approach is not immediately clear. Still on the regions outside the training data each of the trained NN is inaccurate. So what is the benefit you might ask. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbaSI1GNl3Ij"
      },
      "source": [
        "Well let's plot the above in a slightly different way: let's visualize the ensemble's **uncertainty bands**.\n",
        "> From a Bayesian perspective we want to quanity the model's uncertainty on its prediction. This is done via the marginal $p(y|x, D)$, which can be computed as:\n",
        "\n",
        "$$\n",
        "p(y|x, D) = \\int_{\\theta}p(y|x,\\theta')p(\\theta'|D)d\\theta'\n",
        "$$\n",
        "\n",
        "> In practice, for Deep Ensambles we approximate the above by computing the mean and standard deviation across the ensemble. Meaning $p(\\theta|D)$ represents the parameters of one of the trained models, $\\theta_{i} ∼ p(\\theta|D)$, which we then use to compute $y_{i} = f(x,\\theta_{i})$, representing $p(y|x,\\theta')$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVIm2yYyKWBW"
      },
      "outputs": [],
      "source": [
        "def plot_uncertainty_bands(x_test, y_preds):\n",
        "    y_preds = np.array(y_preds)\n",
        "    y_mean = y_preds.mean(axis=0)\n",
        "    y_std = y_preds.std(axis=0)\n",
        "\n",
        "    def add_uncertainty(ax):\n",
        "        ax.plot(x_test, y_mean, '-', linewidth=3, color=\"#408765\", label=\"predictive mean\")\n",
        "        ax.fill_between(x_test.ravel(), y_mean - 2 * y_std, y_mean + 2 * y_std, alpha=0.6, color='#86cfac', zorder=5)\n",
        "\n",
        "    plot_generic(add_uncertainty)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiG5c2INKWBX"
      },
      "outputs": [],
      "source": [
        "plot_uncertainty_bands(x_test, y_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dHpslAt_U1I"
      },
      "source": [
        "Now we see the benefit of a Bayesian approach. Outside the training region we not only have the point estimate, but also model's uncertainty about it's predicition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HrmIHhpt4Pq"
      },
      "source": [
        "### Monte Carlo Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8By-lUBxoEa"
      },
      "source": [
        "First we create our MC-Dropout Network. As you can see in the code below, creating a dropout network is extremely simple:\n",
        "We can reuse our existing network architecture, the only alteration is that during the forward pass we randomly *switch off* (zero) some of the elements of the input tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOUCZH90AHW-"
      },
      "source": [
        "The Bayesian interpretation of MC-Dropout is that we can see each dropout configuration as a different sample from the approximate posterior distribution $\\theta_{i} ∼ q(\\theta|D)$. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2Txp5BqAROc"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nm522N3Ut7HW"
      },
      "outputs": [],
      "source": [
        "net_dropout = MLP(hidden_dim=30, n_hidden_layers=2, use_dropout=True)\n",
        "net_dropout = train(net_dropout, train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93qhxrYKAVf4"
      },
      "source": [
        "**Evaluate**\n",
        "\n",
        "Similarly to Deep Ensembles, we pass the test data multiple times through the MC-Dropout network. We do so to obtain $y_{i}$ at the different parameter settings, $\\theta_{i}$ of the network, $y_{i}=f(x,\\theta_{i})$, governed by the dropout mask.\n",
        "\n",
        ">This is the main difference compared to dropout implementation in a deterministic NN where it serves as a regularization term. In normal dropout application during test time the dropout is **not** applied. Meaning that all connections are present, but the weights are [adjusted](https://cs231n.github.io/neural-networks-2/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pF0ZwbC82pu"
      },
      "outputs": [],
      "source": [
        "n_dropout_samples = 100\n",
        "\n",
        "# compute predictions, resampling dropout mask for each forward pass\n",
        "y_preds = [net_dropout(x_test).clone().detach().numpy() for _ in range(n_dropout_samples)]\n",
        "y_preds = np.array(y_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxNmyDaDuTku"
      },
      "outputs": [],
      "source": [
        "plot_multiple_predictions(x_test, y_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxUZ4VW54U4x"
      },
      "source": [
        "In the above plot each colored line (apart from blue) represents a different parametrization, $\\theta_{i}$, of our MC-Dropout Network. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQUB_-BP9Pi4"
      },
      "source": [
        "Likewise to the Deep Ensemble Network, we can also compute the MC-dropout's **uncertainty bands**.\n",
        "\n",
        "> The appraoch in practice is the same as before: we compute the mean and standard deviation across each dropout mask, which corresponds to the marginal estimation we discussed earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWbWTg4I9Xfs"
      },
      "outputs": [],
      "source": [
        "plot_uncertainty_bands(x_test, y_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhnpvRLoDk3X"
      },
      "source": [
        "In the same way as Deep Ensembles, MC-Dropout allows us to have an uncertainty estimate next to our point wise predictions. However, for the given use-case this has come with the cost of an overall drop in the model's performance on the training regions. We observe this because at every pass through our network we randomly choose which nodes to keep, so one could argue that we hinder the networks optimal performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdNQWhhw_g7M"
      },
      "source": [
        "## Exercise: Detecting Distribution Shift on MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxfGVUSq_lkR"
      },
      "source": [
        "In this exercise we will compare Bayesian NNs with deterministic NNs on a distribution shift detection task. To do this, we'll monitor the predictive entropy as the distribution gradually shifts.  A model with better uncertainty quantification should become less certain---that is, have a more entropic predictive distribuiton---as the input distribution shifts.  Mathematically, our quantity of interest is:\n",
        "$$ \\mathbb{H}[y | x^{*}, D] = - \\sum_{y} p(y | x^{*}, D) \\log p(y | x^{*}, D)$$ where $p(y | x^{*}, D)$ is the predictive distribuiton: $$ p(y | x^{*}, D) = \\int_{\\theta} p(y | x^{*}, \\theta) \\ p(\\theta | D) \\ d \\theta.$$ The goal is to essentially replicate Figure #1 from the paper [Multiplicative Normalizing Flows for Variational Bayesian Neural Networks](https://arxiv.org/abs/1603.04733), comparing MC dropout, ensembles, and a Bayesian NN.  \n",
        "\n",
        "We will be using the well-known MNIST dataset, a set of 70,000 hand-written digit images, and we will generate a gradual distribution shift on the dataset by rotating the images. As such, the final plot will depict the change in the entropy of the predictive distribution (y-axis) as degree of rotation increases (x-axis). The paper above shows the result for one image.  We, on the other hand, will average over multiple images to make a better comparison between models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wmYDSaYKWBZ"
      },
      "source": [
        "We'll use rotation to simulate a smooth shift. Here's how you can rotate a given image: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKPFqHmcKWBa"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from torchvision import datasets\n",
        "from torch.nn.functional import softmax\n",
        "from torchvision.transforms.functional import rotate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFrCUTcCKWBa"
      },
      "outputs": [],
      "source": [
        "def imshow(image):\n",
        "    plt.imshow(image, cmap='gray', vmin=0, vmax=255)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YS072nD9KWBa"
      },
      "outputs": [],
      "source": [
        "def show_rotation_on_mnist_example_image():\n",
        "    mnist_train = datasets.MNIST('../data', train=True, download=True)\n",
        "    image = Image.fromarray(mnist_train.data[0].numpy())\n",
        "    imshow(image)\n",
        "    rotated_image = rotate(image, angle=90)\n",
        "    imshow(rotated_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6u6jUPAKWBa"
      },
      "outputs": [],
      "source": [
        "show_rotation_on_mnist_example_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8_g0_RPKSeu"
      },
      "source": [
        "Let's setup the training and testing data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roXpaiV_KREL"
      },
      "outputs": [],
      "source": [
        "def get_mnist_data(train=True):\n",
        "    mnist_data = datasets.MNIST('../data', train=train, download=True)\n",
        "    x = mnist_data.data.reshape(-1, 28 * 28).float()\n",
        "    y = mnist_data.targets\n",
        "    return x, y\n",
        "\n",
        "x_train, y_train = get_mnist_data(train=True)\n",
        "x_test, y_test = get_mnist_data(train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqs4ZaR8I94J"
      },
      "source": [
        "Now that we have the data, let's start training neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ_ag9zvZN8M"
      },
      "source": [
        "### Deterministic Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uTU-JQ3TMHl"
      },
      "source": [
        "We will reuse our MLP network architecture with different hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDlBDxTxKWBb"
      },
      "outputs": [],
      "source": [
        "net = MLP(input_dim=784, output_dim=10, hidden_dim=30, n_hidden_layers=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euIGNcKvTnoa"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqROdT0AKWBb"
      },
      "outputs": [],
      "source": [
        "def train_on_mnist(net):\n",
        "    x_train, y_train = get_mnist_data(train=True)\n",
        "    optimizer = torch.optim.Adam(params=net.parameters(), lr=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    batch_size = 250\n",
        "\n",
        "    progress_bar = trange(20)\n",
        "    for _ in progress_bar:\n",
        "        for batch_idx in range(int(x_train.shape[0] / batch_size)):\n",
        "            batch_low, batch_high = batch_idx * batch_size, (batch_idx + 1) * batch_size\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(target=y_train[batch_low:batch_high], input=net(x_train[batch_low:batch_high]))\n",
        "            progress_bar.set_postfix(loss=f'{loss / batch_size:.3f}')\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    return net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwbTW4nKKWBb"
      },
      "outputs": [],
      "source": [
        "net = train_on_mnist(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-LYJPHsUAZU"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxY1rjWGFt50"
      },
      "outputs": [],
      "source": [
        "def accuracy(targets, predictions):\n",
        "  return (targets == predictions).sum() / targets.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTIWGFT_ZhPk"
      },
      "outputs": [],
      "source": [
        "def evaluate_accuracy_on_mnist(net):\n",
        "    test_data = get_mnist_data(train=False)\n",
        "    x_test, y_test = test_data\n",
        "    net.eval()\n",
        "    y_preds = net(x_test).argmax(1)\n",
        "    acc = accuracy(y_test, y_preds)\n",
        "    print(\"Test accuracy is %.2f%%\" % (acc.item() * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0H0EhgKdKWBc"
      },
      "outputs": [],
      "source": [
        "evaluate_accuracy_on_mnist(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox3xSKvfUbE8"
      },
      "source": [
        "### Rotating the images\n",
        "\n",
        "Now let's compute predictive entropy on some rotated images..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ok5zcjtUG12"
      },
      "source": [
        "First we will generate the rotated images with an increasing rotation angle from the test images. We use a subset of the MNIST test set for evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwiScyfcKWBd"
      },
      "outputs": [],
      "source": [
        "def get_mnist_test_subset(n_test_images):\n",
        "    mnist_test = datasets.MNIST('../data', train=False, download=True)\n",
        "    x = mnist_test.data[:n_test_images].float()\n",
        "    y = mnist_test.targets[:n_test_images]\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7sO16l8KWBd"
      },
      "outputs": [],
      "source": [
        "x_test_subset, y_test_subset = get_mnist_test_subset(n_test_images=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTP5XStjUfet"
      },
      "outputs": [],
      "source": [
        "rotation_angles = [3 * i for i in range(0, 31)] # use angles from 0 to 90 degrees\n",
        "rotated_images = [rotate(x_test_subset, angle).reshape(-1, 28 * 28) for angle in rotation_angles]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk4Od6dMUQmG"
      },
      "source": [
        "Evaluate the trained MLP on the rotated images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHYpRl6eKWBd"
      },
      "outputs": [],
      "source": [
        "y_preds_deterministic = [softmax(net(images), dim=-1) for images in rotated_images]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAd2CBauKWBd"
      },
      "source": [
        "The [information entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)) $H$ of a probability distribution $p$ over a discrete random variable $X$ with possible outcomes $x_1, \\ldots, x_N$, occuring with probabilities $p(x_i) := p_i$ is given by: $$ H(p) = - \\sum_{i=1}^{N} p_i \\log p_i $$ \n",
        "The entropy quantifies the uncertainty of a probability distribution in the sense, that the more uncertain the outcome a hypothetical experiment with drawing from the distribution is the higher the entropy. Highest is for an equal distribution of probability mass over all possible outcomes.\n",
        "In our case the deterministic NN estimates a probability distribution over the ten digits as classes on MNIST for each image. For the rotated images we can thus calculate the entropy over the rotation angle. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJOPIcgBKWBe"
      },
      "source": [
        "> **1.1 How do you expect the entropy to behave with increasing rotation angle of the images?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpubRC1rKWBe"
      },
      "source": [
        "> **1.2 Implement a function for calculating the entropy according to the formula above.** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1L9ANf6KWBe"
      },
      "outputs": [],
      "source": [
        "def entropy(p):\n",
        "  return NotImplemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teFr3wuwKWBe"
      },
      "source": [
        "Now we can calculate the accuracies and entropies for all rotated images and plot both:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKCHpnYeKWBe"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracies_and_entropies(y_preds):\n",
        "    accuracies = [accuracy(y_test_subset, p.argmax(axis=1)) for p in y_preds]\n",
        "    entropies = [np.mean(entropy(p.detach().numpy())) for p in y_preds]\n",
        "    return accuracies, entropies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVKC6W6DKWBe"
      },
      "outputs": [],
      "source": [
        "def plot_accuracy_and_entropy(add_to_plot):\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    plt.xlim([0, 90])\n",
        "    plt.xlabel(\"Rotation Angle\", fontsize=20)\n",
        "\n",
        "    add_to_plot(ax)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlrXi3LuKWBe"
      },
      "outputs": [],
      "source": [
        "def add_deterministic(ax):\n",
        "    accuracies, entropies = calculate_accuracies_and_entropies(y_preds_deterministic)\n",
        "    ax.plot(rotation_angles, accuracies, 'b--', linewidth=3, label=\"Accuracy, Deterministic\")\n",
        "    ax.plot(rotation_angles, entropies, 'b-', linewidth=3, label=\"Entropy, Deterministic\")\n",
        "plot_accuracy_and_entropy(add_deterministic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe56_xKTrdRf"
      },
      "source": [
        "What is your interpretation of the plot above: Is the predictive entropy changing? If so, how would you explain this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UR0dII0J969"
      },
      "source": [
        "### Monte Carlo Dropout Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G0WBuo5xfMr"
      },
      "source": [
        "Let's create our Dropout Network. We keep the network depth and hidden layer size the same as for the MLP for a fair model comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yP5BbBIJ7Rp"
      },
      "outputs": [],
      "source": [
        "net_dropout = MLP(input_dim=784, output_dim=10, hidden_dim=30, n_hidden_layers=3, use_dropout=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG9CihZfWUEa"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7qUMq0XKWBf"
      },
      "outputs": [],
      "source": [
        "net_dropout = train_on_mnist(net_dropout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O85T4z9EWZf1"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QzVSe3dOrrn",
        "outputId": "b110da55-3b18-44ed-a67c-92a6a7349940"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy is 92.14%\n"
          ]
        }
      ],
      "source": [
        "evaluate_accuracy_on_mnist(net_dropout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT5LNMYCWen7"
      },
      "source": [
        "**Evaluate on rotated images**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GoNZeoYKWBg"
      },
      "source": [
        "> **2.1 Sample 100 different dropout masks and average the predictions over them.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TSZMb0Ccj8i"
      },
      "outputs": [],
      "source": [
        "n_dropout_samples = 100\n",
        "net_dropout.train()  # we set the model to train to 'activate' the dropout layer\n",
        "y_preds_dropout = NotImplemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4j7SNOsKWBg"
      },
      "source": [
        "> **2.2 What is the best way to average over the predictions? Should you first average the network output and then apply the softmax, or the other way around?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OsAQ_oNKWBg"
      },
      "outputs": [],
      "source": [
        "def add_deterministic_and_dropout(ax):\n",
        "    accuracies, entropies = calculate_accuracies_and_entropies(y_preds_deterministic)\n",
        "    ax.plot(rotation_angles, accuracies, 'b--', linewidth=3, label=\"Accuracy, Deterministic\")\n",
        "    ax.plot(rotation_angles, entropies, 'b-', linewidth=3, label=\"Entropy, Deterministic\")\n",
        "    \n",
        "    accuracies, entropies = calculate_accuracies_and_entropies(y_preds_dropout)\n",
        "    ax.plot(rotation_angles, accuracies, 'r--', linewidth=3, label=\"Accuracy, MC Dropout\")\n",
        "    ax.plot(rotation_angles, entropies, 'r-', linewidth=3, label=\"Entropy, MC Dropout\")\n",
        "plot_accuracy_and_entropy(add_deterministic_and_dropout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvhSvZGNWiyG"
      },
      "source": [
        "How does MLP compare with MC-Dropout Network? (Are there any benefits of the Bayesian approach?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI4RJ6sp1T55"
      },
      "source": [
        "### Deep Ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYY-KJogf7lt"
      },
      "source": [
        "Now let's investigate Deep Ensemble performance. We will use the exact same network hyperparameters as for the MLP:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PXtw09YKWBg"
      },
      "source": [
        "> **3.1 Define and train an ensemble of five MLPs with the same hyperparameters as above.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gfeSmgfKWBh"
      },
      "outputs": [],
      "source": [
        "ensemble_size = 5\n",
        "ensemble = NotImplemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHr0jqY-hIQ-"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBei83L9KWBh"
      },
      "outputs": [],
      "source": [
        "ensemble = NotImplemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW2dIXEehXgS"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWAfKX3qKWBh"
      },
      "source": [
        "> **3.2 Evaluate the accuracy of the ensemble prediction. How do you aggregate best over the multiple different predictions given by the members of the ensemble? What is the difference to the regression setting above?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lg6kEvBPKWBh"
      },
      "outputs": [],
      "source": [
        "y_preds = NotImplemented\n",
        "acc = accuracy(y_test, y_preds)\n",
        "print(\"Test accuracy is %.2f%%\" % (acc.item() * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne0DbvGiixm1"
      },
      "source": [
        "**Evaluate on rotated images**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOP3JKG2KWBi"
      },
      "source": [
        "> **3.3 Again, average the predictions, but this time over the members of the ensemble.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iDAcUEgKWBi"
      },
      "outputs": [],
      "source": [
        "y_preds_ensemble = NotImplemented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGJWACI-KWBi"
      },
      "outputs": [],
      "source": [
        "def add_deep_ensemble(ax):\n",
        "    accuracies, entropies = calculate_accuracies_and_entropies(y_preds_deterministic)\n",
        "    ax.plot(rotation_angles, accuracies, 'b--', linewidth=3, label=\"Accuracy, Deterministic\")\n",
        "    ax.plot(rotation_angles, entropies, 'b-', linewidth=3, label=\"Entropy, Deterministic\")\n",
        "\n",
        "    accuracies, entropies = calculate_accuracies_and_entropies(y_preds_dropout)\n",
        "    ax.plot(rotation_angles, accuracies, 'r--', linewidth=3, label=\"Accuracy, MC Dropout\")\n",
        "    ax.plot(rotation_angles, entropies, 'r-', linewidth=3, label=\"Entropy, MC Dropout\")\n",
        "\n",
        "    accuracies, entropies = calculate_accuracies_and_entropies(y_preds_ensemble)\n",
        "    ax.plot(rotation_angles, accuracies, 'g--', linewidth=3, label=\"Accuracy, Deep Ensemble\")\n",
        "    ax.plot(rotation_angles, entropies, 'g-', linewidth=3, label=\"Entropy, Deep Ensemble\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e21gIDl2KWBi"
      },
      "outputs": [],
      "source": [
        "plot_accuracy_and_entropy(add_deep_ensemble)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTdld9_HRUmG"
      },
      "source": [
        "Are there any differences in the performance? Explain why you see or don't see any differences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3PtQMNc1Y3C"
      },
      "source": [
        "### Bayesian Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gaTgZpiaImr"
      },
      "source": [
        "First install pyro package:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLijMIUKaIJQ"
      },
      "outputs": [],
      "source": [
        "!pip install pyro-ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mab-zhWNaNgu"
      },
      "outputs": [],
      "source": [
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "from pyro.nn import PyroModule, PyroSample\n",
        "from pyro.infer import Predictive\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.infer.autoguide import AutoDiagonalNormal\n",
        "from pyro.distributions import Normal, Categorical\n",
        "from torch.nn.functional import softmax\n",
        "from tqdm.auto import trange, tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff_nQKGGKWBi"
      },
      "source": [
        "> **4.1 Implement a Bayesian Neural Network for classifying MNIST digits. For orientation you can use the first tutorial.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kqrd5gz8aQvj"
      },
      "source": [
        "As a backbone use the MLP architecture introduced in the beginning of the notebook. However, because we will implement a custom *guide()*, define every layer explicitly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCezFW_laYf3"
      },
      "outputs": [],
      "source": [
        "class My_MLP(nn.Module):\n",
        "   '''\n",
        "   Implement a MLP with 3 hidden layers\n",
        "   '''\n",
        "    \n",
        "    def __init__(self, in_dim=784, out_dim=10, hid_dim=200):\n",
        "        super().__init__()\n",
        "        assert in_dim > 0\n",
        "        assert out_dim > 0\n",
        "        assert hid_dim > 0\n",
        "\n",
        "        #3 hidden layers\n",
        "        self.fc1 = \n",
        "        self.fc2 = \n",
        "        self.fc3 = \n",
        "        self.out = \n",
        "\n",
        "        raise NotImplemented\n",
        "        \n",
        "    def forward(self, x):\n",
        "        raise NotImplemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmwgEa2ra0kR"
      },
      "source": [
        "Initialize the network. You will have to access it's layers in your model and guide functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMfAuzbba2gc"
      },
      "outputs": [],
      "source": [
        "net = My_MLP()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWNW2gOaa4Gx"
      },
      "outputs": [],
      "source": [
        "#confirm your layer names\n",
        "for name, _ in net.named_parameters():\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e68Tyu7ka7kJ"
      },
      "source": [
        "Define the model:\n",
        "> Probablistic models in Pyro are specified as *model()* functions. This function defines how the output data is generated. Within the model() function, first, the pyro module *random_module()* converts the paramaters of our NN into random variables that have prior probability distributions. Second, in pyro *sample* we define that the output of the network is categorical, while the pyro *plate* allows us to vectorize this function for computational efficiency.\n",
        "\n",
        "> Hint: remember we are doing a classification instead of regression!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaOHlyvnbAK8"
      },
      "source": [
        "You can 'cheat' a little: to speed up the training and limit a bit more the number of paramters we need to optimize, implement a BNN where only the **last layer** is Bayesian!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onKND8Baa5Vc"
      },
      "outputs": [],
      "source": [
        "def model(x_data, y_data):\n",
        "  raise NotImplemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWex-ueobMKJ"
      },
      "source": [
        "implement the guide(), *variational distribution*:\n",
        "> the guide allows us to initialise a well behaved distribution which later we can optmize to approximate the true posterior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "py7SPuMQbOUg"
      },
      "outputs": [],
      "source": [
        "def my_guide(x_data, y_data):\n",
        "  raise NotImplemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19rImW8DbUc3"
      },
      "source": [
        "Initialize the stochastic variational inference (SVI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6TC1s-LbUPk"
      },
      "outputs": [],
      "source": [
        "adam = pyro.optim.Adam({\"lr\": 1e-3})\n",
        "svi = raise NotImplemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kCctWuIbepN"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUL8RC1UbgAd"
      },
      "outputs": [],
      "source": [
        "pyro.clear_param_store()\n",
        "batch_size = 250\n",
        "bar = trange(30)\n",
        "for epoch in bar:\n",
        "  for batch_idx in range(int(x_train.shape[0] / batch_size)):\n",
        "    batch_low, batch_high = batch_idx * batch_size, (batch_idx+1) * batch_size\n",
        "    loss = svi.step(x_train[batch_low:batch_high], y_train[batch_low:batch_high])\n",
        "    bar.set_postfix(loss=f'{loss / batch_size:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRrGGad3blsw"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9amgVkv6bohg"
      },
      "source": [
        "Use the learned *guide()* function to do predictions. Why? Because the *model()* function knows the **priors** for the weights and biases, **not** the learned posterior. The *guide()* contains the approximate posterior distributions of the parameter values, which we want to use to make the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8zI0pQrblDE"
      },
      "outputs": [],
      "source": [
        "y_preds = NotImplemented\n",
        "acc = accuracy(y_test, y_preds)\n",
        "print(\"Test accuracy is %.2f%%\" % (acc.item() * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xuKhT3rb1S8"
      },
      "source": [
        "**Evaluate on rotated images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xo6B-szob9Tt"
      },
      "outputs": [],
      "source": [
        "y_preds_bnn = NotImplemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_DkEY1K2Efb"
      },
      "source": [
        "> **4.2 Show entropies for all four models. Which method is the best at detecting the distribution shift? How can you interpret this?**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DO2PLygwbkLW"
      },
      "outputs": [],
      "source": [
        "#add the computed values for BNN\n",
        "def add_bnn():\n",
        "  raise NotImplemented\n",
        "plot_accuracy_and_entropy(add_bnn)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Student DLII - BNN 2/2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide 4: Research Projects with JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Any, Sequence\n",
    "import datetime\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "# JAX/Flax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state, checkpoints\n",
    "import optax\n",
    "# Logging with Tensorboard or Weights and Biases\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    # A simple extension of TrainState to also include batch statistics\n",
    "    # If a model has no batch statistics, it is None\n",
    "    batch_stats : Any = None\n",
    "    # You can further extend the TrainState by any additional part here\n",
    "    # For example, rng to keep for init, dropout, etc.\n",
    "    rng : Any = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerModule:\n",
    "\n",
    "    def __init__(self, \n",
    "                 model_class : nn.Module,\n",
    "                 model_hparams : dict,\n",
    "                 optimizer_hparams : dict,\n",
    "                 exmp_input : Any,\n",
    "                 seed : int = 42,\n",
    "                 logger_params : dict = None,\n",
    "                 cluster : bool = False,\n",
    "                 debug : bool = False,\n",
    "                 check_val_every_n_epoch : int = 1,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.model_class = model_class\n",
    "        self.model_hparams = model_hparams\n",
    "        self.optimizer_hparams = optimizer_hparams\n",
    "        self.cluster = cluster\n",
    "        self.debug = debug\n",
    "        self.seed = seed\n",
    "        self.check_val_every_n_epoch = check_val_every_n_epoch\n",
    "        self.exmp_input = exmp_input\n",
    "        self.config = {\n",
    "            'model_class': model_class.__name__,\n",
    "            'model_hparams': model_hparams,\n",
    "            'optimizer_hparams': optimizer_hparams,\n",
    "            'logger_params': logger_params,\n",
    "            'cluster': self.cluster,\n",
    "            'debug': self.debug,\n",
    "            'seed': self.seed\n",
    "        }\n",
    "        self.config.update(kwargs)\n",
    "        self.has_batch_norm = False\n",
    "        # Create empty model. Note: no parameters yet\n",
    "        self.model = self.model_class(**self.model_hparams)\n",
    "        self.print_tabulate(exmp_input)\n",
    "        # Init trainer parts\n",
    "        self.init_logger(logger_params)\n",
    "        self.create_jitted_functions()\n",
    "        self.init_model(exmp_input)\n",
    "\n",
    "    def init_logger(self, logger_params):\n",
    "        if logger_params is None:\n",
    "            logger_params = dict()\n",
    "        \n",
    "        base_log_dir = logger_params.get('base_log_dir', 'checkpoints/')\n",
    "        # Prepare logging\n",
    "        log_dir = os.path.join(base_log_dir, self.config[\"model_class\"])\n",
    "        if 'logger_name' in logger_params:\n",
    "            log_dir = os.path.join(log_dir, logger_params['logger_name'])\n",
    "        \n",
    "        logger_type = logger_params.get('logger_type', 'TensorBoard').lower()\n",
    "        if logger_type == 'tensorboard':\n",
    "            self.logger = TensorBoardLogger(save_dir=log_dir, \n",
    "                                            name='')\n",
    "        elif logger_type == 'wandb':\n",
    "            self.logger = WandbLogger(name=logger_params.get('project_name', None),\n",
    "                                      save_dir=log_dir, \n",
    "                                      config=self.config)\n",
    "        else:\n",
    "            assert False, f'Unknown logger type \\\"{logger_type}\\\"'\n",
    "        log_dir = self.logger.log_dir\n",
    "        os.makedirs(os.path.join(log_dir, 'metrics/'), exist_ok=True)\n",
    "        with open(os.path.join(log_dir, 'hparams.json'), 'w') as f:\n",
    "            json.dump(self.config, f, indent=4)\n",
    "        self.log_dir = log_dir\n",
    "    \n",
    "    def create_jitted_functions(self):\n",
    "        train_step, eval_step = self.create_functions()\n",
    "        if self.debug:  # Skip jitting \n",
    "            print('Skipping jitting due to debug=True')\n",
    "            self.train_step = train_step\n",
    "            self.eval_step = eval_step\n",
    "        else:\n",
    "            self.train_step = jax.jit(train_step)\n",
    "            self.eval_step = jax.jit(eval_step)\n",
    "\n",
    "    def create_functions(self):\n",
    "        def train_step(state, batch):\n",
    "            metrics = {}\n",
    "            return state, metrics\n",
    "        def eval_step(state, batch):\n",
    "            metrics = {}\n",
    "            return metrics\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def init_model(self, exmp_input):\n",
    "        # Initialize model\n",
    "        model_rng = random.PRNGKey(self.seed)\n",
    "        model_rng, init_rng = random.split(model_rng)\n",
    "        exmp_input = [exmp_input] if not isinstance(exmp_input, (list, tuple)) else exmp_input\n",
    "        variables = self.run_model_init(exmp_input, init_rng)\n",
    "        self.init_params = variables['params']\n",
    "        self.init_batch_stats = variables.get('batch_stats')  # Returns none if no batch stats exist\n",
    "        self.state = TrainState(step=0, \n",
    "                                apply_fn=self.model.apply,\n",
    "                                params=variables['params'],\n",
    "                                batch_stats=variables.get('batch_stats'),\n",
    "                                rng=model_rng,\n",
    "                                tx=None,\n",
    "                                opt_state=None)\n",
    "\n",
    "    def run_model_init(self, exmp_input, init_rng):\n",
    "        return self.model.init(init_rng, *exmp_input, train=True)\n",
    "\n",
    "    def print_tabulate(self, exmp_input):\n",
    "        tabulate_fn = nn.tabulate(self.model, random.PRNGKey(0))\n",
    "        print(tabulate_fn(*exmp_input, train=True))\n",
    "\n",
    "    def init_optimizer(self, num_epochs, num_steps_per_epoch):\n",
    "        hparams = copy(self.optimizer_hparams)\n",
    "\n",
    "        # Initialize learning rate schedule and optimizer\n",
    "        optimizer_name = hparams.pop('optimizer', 'adamw')\n",
    "        if optimizer_name.lower() == 'adam':\n",
    "            opt_class = optax.adam\n",
    "        elif optimizer_name.lower() == 'adamw':\n",
    "            opt_class = optax.adamw\n",
    "        elif optimizer_name.lower() == 'sgd':\n",
    "            opt_class = optax.sgd\n",
    "        else:\n",
    "            assert False, f'Unknown optimizer \"{opt_class}\"'\n",
    "        \n",
    "        lr = hparams.pop('lr', 1e-3)\n",
    "        warmup = hparams.pop('warmup', 0)\n",
    "        lr_schedule = optax.warmup_cosine_decay_schedule(\n",
    "            init_value=0.0,\n",
    "            peak_value=lr,\n",
    "            warmup_steps=warmup,\n",
    "            decay_steps=int(num_epochs * num_steps_per_epoch),\n",
    "            end_value=0.01 * lr\n",
    "        )\n",
    "        # Clip gradients at max value, and evt. apply weight decay\n",
    "        transf = [optax.clip_by_global_norm(hparams.pop('gradient_clip', 1.0))]\n",
    "        if opt_class == optax.sgd and 'weight_decay' in hparams:  # wd is integrated in adamw\n",
    "            transf.append(optax.add_decayed_weights(hparams.pop('weight_decay', 0.0)))\n",
    "        optimizer = optax.chain(\n",
    "            *transf,\n",
    "            opt_class(lr_schedule, **hparams)\n",
    "        )\n",
    "        # Initialize training state\n",
    "        self.state = TrainState.create(apply_fn=self.state.apply_fn,\n",
    "                                       params=self.state.params,\n",
    "                                       batch_stats=self.state.batch_stats,\n",
    "                                       tx=optimizer,\n",
    "                                       rng=self.state.rng)\n",
    "\n",
    "    def train_model(self, train_loader, val_loader, test_loader=None, num_epochs=500):\n",
    "        # Train model for defined number of epochs\n",
    "        # We first need to create optimizer and the scheduler for the given number of epochs\n",
    "        self.init_optimizer(num_epochs, len(train_loader))\n",
    "        self.on_training_start()\n",
    "        best_eval_metrics = None\n",
    "        for epoch_idx in self.tracker(range(1, num_epochs+1), desc='Epochs'):\n",
    "            train_metrics = self.train_epoch(train_loader)\n",
    "            self.logger.log_metrics(train_metrics, step=epoch_idx)\n",
    "            self.on_training_epoch_end(epoch_idx)\n",
    "            if epoch_idx % self.check_val_every_n_epoch == 0:\n",
    "                eval_metrics = self.eval_model(val_loader, log_prefix='val/')\n",
    "                self.on_validation_epoch_end(epoch_idx, eval_metrics, val_loader)\n",
    "                self.logger.log_metrics(eval_metrics, step=epoch_idx)\n",
    "                self.save_metrics(f'eval_epoch_{str(epoch_idx).zfill(3)}', eval_metrics)\n",
    "                if self.is_new_model_better(eval_metrics, best_eval_metrics):\n",
    "                    best_eval_metrics = eval_metrics\n",
    "                    self.save_model(step=epoch_idx)\n",
    "                    self.save_metrics('best_eval', eval_metrics)\n",
    "        if test_loader is not None:\n",
    "            self.load_model()\n",
    "            test_metrics = self.eval_model(test_loader, log_prefix='test/')\n",
    "            self.logger.log_metrics(test_metrics, step=epoch_idx)\n",
    "            self.save_metrics('test', test_metrics)\n",
    "            best_eval_metrics.update(test_metrics)\n",
    "        return best_eval_metrics\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        # Train model for one epoch, and log avg loss and accuracy\n",
    "        metrics = defaultdict(float)\n",
    "        num_train_steps = len(train_loader)\n",
    "        for batch in self.tracker(train_loader, desc='Training', leave=False):\n",
    "            self.state, step_metrics = self.train_step(self.state, batch)\n",
    "            for key in step_metrics:\n",
    "                metrics['train/' + key] += step_metrics[key] / num_train_steps\n",
    "        metrics = jax.device_get(metrics)\n",
    "        return metrics\n",
    "\n",
    "    def eval_model(self, data_loader, log_prefix=''):\n",
    "        # Test model on all images of a data loader and return avg loss\n",
    "        metrics = defaultdict(float)\n",
    "        num_elements = 0\n",
    "        for batch in data_loader:\n",
    "            step_metrics = self.eval_step(self.state, batch)\n",
    "            batch_size = batch[0].shape[0] if isinstance(batch, (list, tuple)) else batch.shape[0]\n",
    "            for key in step_metrics:\n",
    "                metrics[key] += step_metrics[key] * batch_size\n",
    "            num_elements += batch_size\n",
    "        metrics = {(log_prefix + key): (metrics[key] / num_elements).item() for key in metrics}\n",
    "        return metrics\n",
    "\n",
    "    def tracker(self, iterator, **kwargs):\n",
    "        if not self.cluster:\n",
    "            return tqdm(iterator, **kwargs)\n",
    "        else:\n",
    "            return iterator\n",
    "\n",
    "    def is_new_model_better(self, new_metrics, old_metrics):\n",
    "        if old_metrics is None:\n",
    "            return True\n",
    "        for key, is_larger in [('val/val_metric', False), ('val/acc', True), ('val/loss', False)]:\n",
    "            if key in new_metrics:\n",
    "                if is_larger:\n",
    "                    return new_metrics[key] > old_metrics[key]\n",
    "                else:\n",
    "                    return new_metrics[key] < old_metrics[key]\n",
    "        assert False, f'No known metrics to log on: {new_metrics}'\n",
    "\n",
    "    def save_metrics(self, filename, metrics):\n",
    "        with open(os.path.join(self.log_dir, f'metrics/{filename}.json'), 'w') as f:\n",
    "            json.dump(metrics, f, indent=4)\n",
    "\n",
    "    def on_training_start(self):\n",
    "        pass\n",
    "\n",
    "    def on_training_epoch_end(self, epoch_idx):\n",
    "        pass\n",
    "\n",
    "    def on_validation_epoch_end(self, epoch_idx, eval_metrics, val_loader):\n",
    "        pass\n",
    "\n",
    "    def save_model(self, step=0):\n",
    "        # Save current model at certain training iteration\n",
    "        checkpoints.save_checkpoint(ckpt_dir=self.log_dir,\n",
    "                                    target={'params': self.state.params,\n",
    "                                            'batch_stats': self.state.batch_stats},\n",
    "                                    step=step,\n",
    "                                    overwrite=True)\n",
    "\n",
    "    def load_model(self):\n",
    "        # Load model.\n",
    "        state_dict = checkpoints.restore_checkpoint(ckpt_dir=self.log_dir, target=None)\n",
    "        self.state = TrainState.create(apply_fn=self.model.apply,\n",
    "                                       params=state_dict['params'],\n",
    "                                       batch_stats=state_dict['batch_stats'],\n",
    "                                       tx=self.state.tx if self.state else optax.sgd(0.1),   # Default optimizer\n",
    "                                       rng=self.state.rng\n",
    "                                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainState(step=0, apply_fn=None, params=None, tx=None, opt_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "\n",
    "DATASET_PATH = '../data/'\n",
    "CHECKPOINT_PATH = '../saved_models/guide4/'\n",
    "\n",
    "# Transformations applied on each image => bring them into a numpy array\n",
    "DATA_MEANS = np.array([0.49139968, 0.48215841, 0.44653091])\n",
    "DATA_STD = np.array([0.24703223, 0.24348513, 0.26158784])\n",
    "def image_to_numpy(img):\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "    img = (img / 255. - DATA_MEANS) / DATA_STD\n",
    "    return img\n",
    "\n",
    "# We need to stack the batch elements\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "\n",
    "test_transform = image_to_numpy\n",
    "# For training, we add some augmentation. Networks are too powerful and would overfit.\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      image_to_numpy\n",
    "                                     ])\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "# We need to do a little trick because the validation set should not use the augmentation.\n",
    "train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n",
    "val_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\n",
    "train_set, _ = data.random_split(train_dataset, [45000, 5000], generator=torch.Generator().manual_seed(42))\n",
    "_, val_set = data.random_split(val_dataset, [45000, 5000], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Loading the test set\n",
    "test_set = CIFAR10(root=DATASET_PATH, train=False, transform=test_transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for training and validation\n",
    "train_loader = data.DataLoader(train_set,\n",
    "                               batch_size=128,\n",
    "                               shuffle=True,\n",
    "                               drop_last=True,\n",
    "                               collate_fn=numpy_collate,\n",
    "                               num_workers=8,\n",
    "                               persistent_workers=True,\n",
    "                               generator=torch.Generator().manual_seed(42))\n",
    "val_loader   = data.DataLoader(val_set,\n",
    "                               batch_size=128,\n",
    "                               shuffle=False,\n",
    "                               drop_last=False,\n",
    "                               collate_fn=numpy_collate,\n",
    "                               num_workers=4,\n",
    "                               persistent_workers=True)\n",
    "test_loader  = data.DataLoader(test_set,\n",
    "                               batch_size=128,\n",
    "                               shuffle=False,\n",
    "                               drop_last=False,\n",
    "                               collate_fn=numpy_collate,\n",
    "                               num_workers=4,\n",
    "                               persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    hidden_dims : Sequence[int]\n",
    "    num_classes : int\n",
    "    dropout_prob : float = 0.0\n",
    "        \n",
    "    @nn.compact\n",
    "    def __call__(self, x, train=True):\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        for dims in self.hidden_dims:\n",
    "            x = nn.Dropout(self.dropout_prob)(x, deterministic=not train)\n",
    "            x = nn.Dense(dims)(x)\n",
    "            x = nn.BatchNorm()(x, use_running_average=not train)\n",
    "            x = nn.swish(x)\n",
    "        x = nn.Dropout(self.dropout_prob)(x, deterministic=not train)\n",
    "        x = nn.Dense(self.num_classes)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassTrainer(TrainerModule):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 hidden_dims : Sequence[int],\n",
    "                 num_classes : int,\n",
    "                 dropout_prob : float,\n",
    "                 trial : Any = None,\n",
    "                 **kwargs):\n",
    "        super().__init__(model_class=MLPClassifier,\n",
    "                         model_hparams={\n",
    "                             'hidden_dims': hidden_dims,\n",
    "                             'num_classes': num_classes,\n",
    "                             'dropout_prob': dropout_prob\n",
    "                         },\n",
    "                         **kwargs)\n",
    "        self.trial = trial\n",
    "    \n",
    "    def create_functions(self):\n",
    "        def loss_function(params, batch_stats, rng, batch, train):\n",
    "            imgs, labels = batch\n",
    "            labels_onehot = jax.nn.one_hot(labels, num_classes=self.model.num_classes)\n",
    "            rng, dropout_rng = random.split(rng)\n",
    "            output = self.model.apply({'params': params, 'batch_stats': batch_stats},\n",
    "                                      imgs,\n",
    "                                      train=train,\n",
    "                                      rngs={'dropout': dropout_rng},\n",
    "                                      mutable=['batch_stats'] if train else False)\n",
    "            logits, new_model_state = output if train else (output, None)\n",
    "            loss = optax.softmax_cross_entropy(logits, labels_onehot).mean()\n",
    "            acc = (logits.argmax(axis=-1) == labels).mean()\n",
    "            return loss, (rng, new_model_state, acc)\n",
    "        \n",
    "        def train_step(state, batch):\n",
    "            loss_fn = lambda params: loss_function(params, state.batch_stats, state.rng, batch, train=True)\n",
    "            ret, grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "            loss, rng, new_model_state, acc = ret[0], *ret[1]\n",
    "            state = state.apply_gradients(grads=grads, batch_stats=new_model_state['batch_stats'], rng=rng)\n",
    "            metrics = {'loss': loss, 'acc': acc}\n",
    "            return state, metrics\n",
    "        \n",
    "        def eval_step(state, batch):\n",
    "            _, (_, _, acc) = loss_function(state.params, state.batch_stats, state.rng, batch, train=False)\n",
    "            return {'acc': acc}\n",
    "        \n",
    "        return train_step, eval_step\n",
    "    \n",
    "    def run_model_init(self, exmp_input, init_rng):\n",
    "        imgs, _ = exmp_input\n",
    "        init_rng, dropout_rng = random.split(init_rng)\n",
    "        return self.model.init({'params': init_rng, 'dropout': dropout_rng}, x=imgs, train=True)\n",
    "    \n",
    "    def print_tabulate(self, exmp_input):\n",
    "        imgs, _ = exmp_input\n",
    "        print(self.model.tabulate(rngs={'params': random.PRNGKey(0), 'dropout': random.PRNGKey(0)}, x=imgs, train=True))\n",
    "        \n",
    "    def on_validation_epoch_end(self, epoch_idx, eval_metrics, val_loader):\n",
    "        if self.trial:\n",
    "            self.trial.report(eval_metrics['val/acc'], step=epoch_idx)\n",
    "            if self.trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MLPClassTrainer(hidden_dims=[512, 512],\n",
    "                          num_classes=10,\n",
    "                          dropout_prob=0.4,\n",
    "                          optimizer_hparams={\n",
    "                              'weight_decay': 2e-4\n",
    "                          },\n",
    "                          logger_params={\n",
    "                              'base_log_dir': CHECKPOINT_PATH\n",
    "                          },\n",
    "                          exmp_input=next(iter(train_loader)),\n",
    "                          check_val_every_n_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = trainer.train_model(train_loader, \n",
    "#                               val_loader, \n",
    "#                               test_loader=test_loader, \n",
    "#                               num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Validation accuracy: {metrics[\"val/acc\"]:4.2%}')\n",
    "# print(f'Test accuracy: {metrics[\"test/acc\"]:4.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic hyperparameter tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    my_train_loader = data.DataLoader(train_set,\n",
    "                                      batch_size=128,\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=True,\n",
    "                                      collate_fn=numpy_collate,\n",
    "                                      num_workers=8,\n",
    "                                      persistent_workers=True,\n",
    "                                      generator=torch.Generator().manual_seed(42))\n",
    "    my_val_loader = data.DataLoader(val_set,\n",
    "                                    batch_size=128,\n",
    "                                    shuffle=False,\n",
    "                                    drop_last=False,\n",
    "                                    collate_fn=numpy_collate,\n",
    "                                    num_workers=4,\n",
    "                                    persistent_workers=True)\n",
    "    trainer = MLPClassTrainer(hidden_dims=[512, 512],\n",
    "                              num_classes=10,\n",
    "                              dropout_prob=trial.suggest_float('dropout_prob', 0, 0.6),\n",
    "                              optimizer_hparams={\n",
    "                                  'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True),\n",
    "                                  'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "                              },\n",
    "                              logger_params={\n",
    "                                  'base_log_dir': CHECKPOINT_PATH\n",
    "                              },\n",
    "                              exmp_input=next(iter(my_loader)),\n",
    "                              check_val_every_n_epoch=5,\n",
    "                              trial=trial)\n",
    "    metrics = trainer.train_model(my_loader,\n",
    "                                  my_val_loader,\n",
    "                                  num_epochs=200)\n",
    "    my_loader._iterator._shutdown_workers()\n",
    "    my_val_loader._iterator._shutdown_workers()\n",
    "    del trainer\n",
    "    del my_loader, my_val_loader\n",
    "    return metrics['val/acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    study_name='mlp_cifar10',\n",
    "    storage=f'sqlite:///{CHECKPOINT_PATH}/optuna_hparam_search.db',\n",
    "    direction='maximize',\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=50),\n",
    "    load_if_exists=True\n",
    ")\n",
    "study.optimize(objective, n_trials=100, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = study.best_trial\n",
    "print(f'Best Value: {trial.value}')\n",
    "print(f'Best Params:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'-> {key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[![Star our repository](https://img.shields.io/static/v1.svg?logo=star&label=⭐&message=Star%20Our%20Repository&color=yellow)](https://github.com/phlippe/uvadlc_notebooks/)  If you found this tutorial helpful, consider ⭐-ing our repository.    \n",
    "[![Ask questions](https://img.shields.io/static/v1.svg?logo=star&label=❔&message=Ask%20Questions&color=9cf)](https://github.com/phlippe/uvadlc_notebooks/issues)  For any questions, typos, or bugs that you found, please raise an issue on GitHub. \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

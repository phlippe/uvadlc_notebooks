{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide 2: Research projects with PyTorch\n",
    "\n",
    "![Status](https://img.shields.io/static/v1.svg?label=Status&message=First%20version&color=yellow)\n",
    "\n",
    "* Based on some feedback I got, we will try to summarize tips and tricks on how to setup and structure large research projects in PyTorch, such as your Master Thesis\n",
    "* Feel free to contribute yourself if you have good ideas\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Framework\n",
    "\n",
    "* Choose the right framework. If you have simple setups like classification, consider going with PyTorch Lightning. If you need to change the default training procedure, go with plain PyTorch and write your own framework\n",
    "* Usually a good setup:\n",
    "\n",
    "```bash\n",
    "general/\n",
    "│   train.py\n",
    "│   task.py\n",
    "│   mutils.py\n",
    "layers/\n",
    "experiments/\n",
    "│   task1/\n",
    "│        train.py\n",
    "│        task.py\n",
    "│        eval.py\n",
    "│        dataset.py\n",
    "│   task2/\n",
    "│        train.py\n",
    "│        task.py\n",
    "│        eval.py\n",
    "│        dataset.py\n",
    "```\n",
    "\n",
    "* The `general/train.py` file summarizes the default operations every model needs (training loop, loading/saving model, setting up model, etc.). If you use PyTorch Lightning, this reduces to a train file per task, and only needs the specification of the trainer object.\n",
    "* The `general/task.py` file summarizes a template for the specific parts you have to do for a task (training step, validation step, etc.). If you use PyTorch Lightning, this would be the definition of the Lightning Module.\n",
    "* The `layers/models` folder contains the code for specifying the `nn.Modules` you use for setting up the model\n",
    "* The `experiments` folder contains the task-specific code. Each task has its own `train.py` for specifying the argument parser, setting up the model, etc., while the `task.py` overwrites the template in `general/task.py`. The `eval.py` file should has as input a checkpoint directory of a trained model, and should evaluate this model on the test dataset. Finally, the file `dataset.py` contains all parts you need for setting up the dataset.\n",
    "* Note that this template assumes that you might have multiple different tasks and multiple different models. If you have a simpler setup, you can inherintly shrink the template together.\n",
    "\n",
    "\n",
    "### Argument parser\n",
    "\n",
    "* It is a good practice to use argument parsers for specifying hyperparameters. Argument parsers allow you to call a training like `python train.py --learning ... --seed ... --hidden_size ...` etc. \n",
    "* If you have multiple models to choose from, you will have multiple set of hyperparameters. A good summary on that can be found in the [PyTorch Lightning documentation](https://pytorch-lightning.readthedocs.io/en/latest/hyperparameters.html#argparser-best-practices) without the need of using Lightning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search \n",
    "\n",
    "* In general, hyperparameter search is all about experience. Once you have trained a lot of models, it will become easier for you to pick reasonable first-guess hyperparameters.\n",
    "* Another good approach is to look at related work to your model, and see what others have used as hyperparameters for similar models. This will help you to get started with a reasonable choice.\n",
    "* Hyperparameter search can be expensive. Thus, try to do the search on shallow models first before scaling them up.\n",
    "* Although a large grid search is the best way to get the optimum out of your model, it is often not reasonable to run. Try to group hyperparameters, and optimize each group one by one. \n",
    "\n",
    "### Toolkits\n",
    "\n",
    "* PyTorch Lightning provides a lot of useful tricks and toolkits, such as:\n",
    "    * [Learning rate finder](https://pytorch-lightning.readthedocs.io/en/latest/lr_finder.html) that plots the learning rate vs loss for a few initial batches, and helps you to choose a reasonable learning rate.\n",
    "    * [Autoscaling batch sizes](https://pytorch-lightning.readthedocs.io/en/latest/training_tricks.html#auto-scaling-of-batch-size) which finds the largest possible batch size given your GPU (helpful if you have very deep, large models, and it is obvious you need the largest batch size possible)\n",
    "* For comparing multiple hyperparameter configurations, you can add them to TensorBoard. This is a clean way of comparing multiple runs. If interested, a blog on this can be found [here](https://towardsdatascience.com/a-complete-guide-to-using-tensorboard-with-pytorch-53cb2301e8c3)\n",
    "* There are multiple libraries that support you in automatic hyperparameter search. A good overview for those in PyTorch can be found [here](https://medium.com/pytorch/accelerate-your-hyperparameter-optimization-with-pytorchs-ecosystem-tools-bc17001b9a49)\n",
    "\n",
    "### Reproducibility\n",
    "\n",
    "* Everything is about reproducibility. Make sure you can reproduce any training you do with the same random values, batches, etc. You will come to a point where you have tried a lot of different approaches, but none were able to improve upon one of your previous runs. When you try to run the model again with the best hyperparameters, you don't want to have a bad surprise (believe me, enough people have this issue, and it can also happen to you). Hence, before starting any grid search, make sure you are able to reproduce runs. Run two jobs in parallel on Lisa with the same hyperparams, seeds, etc., and if you don't get the exact same results, stop and try to fix it before anything else.\n",
    "* Another fact about reproducibility is that saving and loading a model works without any problems. Make sure before a long training that you are able to load a saved model from the disk, and achieve the exact same test score as you had during training.\n",
    "* Print your hyperparameters into the SLURM output file (simple print statement in python). This will help you identifying the runs, and you can easily check whether Lisa executes the job you intended to\n",
    "* When running a job, copy the job file automatically to your checkpoint folder. Improves repoducibility\n",
    "* Besides the slurm output file, create a output file in which you store the best training, validation and test score. This helps when you want to compare \n",
    "\n",
    "### Seeds\n",
    "\n",
    "* DL models are noisy. Before running a grid search, try to get a feeling of how noisy your experiments might be. The more noise you expect compared to \n",
    "* After finishing the grid search, run another model of the best configuration with a new seed. If the score is still the best, take the model. If not, consider running a few more seeds for the top $k$ models in your grid search. Otherwise you risk taking a suboptimal model, which was just lucky to the best for a specific seed.\n",
    "\n",
    "### Learning rate\n",
    "\n",
    "* Depends on optimizer, model and many more other hyperparameters\n",
    "* A usual good starting point for SGD is 0.1, and Adam 1e-3\n",
    "* The deeper the model is, the lower the learning rate usually should be\n",
    "* The lower your batch, the lower the lr should be. Consider using [gradient accumulation](https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa) if your batch size is getting too small (PyTorch Lightning supports this, see [here](https://pytorch-lightning.readthedocs.io/en/latest/training_tricks.html#accumulate-gradients)). \n",
    "* Consider using the PyTorch Lightning [learning rate finder](https://pytorch-lightning.readthedocs.io/en/latest/lr_finder.html) toolkit for an initial good guess. \n",
    "\n",
    "#### LR scheduler\n",
    "\n",
    "* It again depends on the classifier and model\n",
    "* For classifiers and SGD, the multi-step LR has shown to be good\n",
    "* Models trained with Adam commonly use a smooth exponential decay in the learning rate\n",
    "* For Transformers: remember to use a learning rate warmup, the cosine scheduler is often used for decaying the learning rate afterwards\n",
    "\n",
    "### Regularization\n",
    "\n",
    "* Regularization is important in networks if you see a significant higher training performance than test performance\n",
    "* The regularization parameters all interact with each other, and hence must be tuned together. The most commonly used regularization techniques are: \n",
    "    * Weight decay\n",
    "    * Dropout\n",
    "    * Augmentation\n",
    "* Dropout is usually a good idea as it is applicable to most architectures and has shown to effectively reduce overfitting\n",
    "* If you want to use weight decay in Adam, remember to use `torch.optim.AdamW` instead of `torch.optim.Adam`\n",
    "\n",
    "#### Domain specific regularization\n",
    "\n",
    "* There are couple of regularization techniques that depend on your input data/domain. The most common include:\n",
    "    * Computer Vision: image augmentation\n",
    "    * NLP: input dropout of whole words\n",
    "    * Graphs: dropping edges, inputs\n",
    "\n",
    "\n",
    "### Grid search with SLURM \n",
    "\n",
    "* SLURM supports you do to grid search with [job arrays](https://help.rc.ufl.edu/doc/SLURM_Job_Arrays).\n",
    "* Job arrays allow you to start N jobs in parallel, each running with slightly different settings.\n",
    "* It is effectively the same as creating N job files and calling N times `sbatch ...`, but this can become annoying and is messy at some point.\n",
    "\n",
    "#### Job arrays\n",
    "\n",
    "Job arrays are created with two files: a job file, and a hyperparameter file.\n",
    "The job file will start multiple sub-jobs that each use a different set of hyperparameters, as specified in the hyperparameter file.\n",
    "In the job file, you need to add the argument `#SBATCH --array=...`. The argument specifies how many sub-jobs you want to start, how many to run in parallel (at maximum), and which lines to use from the hyperparameter file.\n",
    "For example, if we specify `#SBATCH --array=1-16%8`, this means that we start 16 jobs using the lines 1 to 16 in the hyperparameter file, and running at maximum 8 jobs in parallel at the same time.\n",
    "Note that the number of parallel jobs is there to limit yourself from blocking the whole cluster.\n",
    "However, with your student accounts, you will not be able to run more than 1 job in parallel anyways.\n",
    "The template job file `array_job.job` looks slightly different than the one we had before. \n",
    "The slurm output file is specified using `%A` and `%a`. `%A` is being automatically replaced with the job ID, while `%a` is the index of the job within the array (so 1 to 16 in our example above).\n",
    "Below, we also added a block for creating a checkpoint folder for the job array, and copying the job file including hyperparameters to that folder.\n",
    "This is good practice for ensuring reproducibility. \n",
    "Finally, in the training call, we specify the path checkpoint path (make sure to have implemented this argument in your argparse) with the addition of `experiment_${SLURM_ARRAY_TASK_ID}` which is a sub-folder in the checkpoint directory with the sub-job ID (1 to 16 in the example).\n",
    "The next line, `$(head -$SLURM_ARRAY_TASK_ID $HPARAMS_FILE | tail -1)`, copies the N-th line of the hyperparameter file to this job file, and hence submits the hyperparameter arguments to the training file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File `array_job.job`:\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --partition=gpu_shared_course\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --job-name=ExampleArrayJob\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=3\n",
    "#SBATCH --time=04:00:00\n",
    "#SBATCH --mem=32000M\n",
    "#SBATCH --array=1-16%8\n",
    "#SBATCH --output=slurm_array_testing_%A_%a.out\n",
    "\n",
    "module purge\n",
    "module load 2019\n",
    "module load Python/3.7.5-foss-2019b\n",
    "module load CUDA/10.1.243\n",
    "module load cuDNN/7.6.5.32-CUDA-10.1.243\n",
    "module load NCCL/2.5.6-CUDA-10.1.243\n",
    "module load Anaconda3/2018.12\n",
    "\n",
    "# Your job starts in the directory where you call sbatch\n",
    "cd $HOME/...\n",
    "# Activate your environment\n",
    "source activate ...\n",
    "\n",
    "# Good practice: define your directory where to save the models, and copy the job file to it\n",
    "JOB_FILE=$HOME/.../array_job.job\n",
    "HPARAMS_FILE=$HOME/.../array_job_hyperparameters.txt\n",
    "CHECKPOINTDIR=$HOME/.../checkpoints/array_job_${SLURM_ARRAY_JOB_ID}\n",
    "\n",
    "mkdir $CHECKPOINTDIR\n",
    "rsync $HPARAMS_FILE $CHECKPOINTDIR/\n",
    "rsync $JOB_FILE $CHECKPOINTDIR/\n",
    "\n",
    "# Run your code\n",
    "srun python -u train.py \\\n",
    "               --checkpoint_path $CHECKPOINTDIR/experiment_${SLURM_ARRAY_TASK_ID} \\\n",
    "\t\t\t   $(head -$SLURM_ARRAY_TASK_ID $HPARAMS_FILE | tail -1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter file is nothing else than a text file in which each line denotes one set of hyperparameters for which you want to run an experiment. There is no specific order in which you need to put the lines, and you can extend the lines with as many hyperparameter arguments as you want.\n",
    "\n",
    "File `array_job_hyperparameters.txt`:\n",
    "```bash\n",
    "--seed 42 --learning_rate 1e-3\n",
    "--seed 43 --learning_rate 1e-3\n",
    "--seed 44 --learning_rate 1e-3\n",
    "--seed 45 --learning_rate 1e-3\n",
    "--seed 42 --learning_rate 2e-3\n",
    "--seed 43 --learning_rate 2e-3\n",
    "--seed 44 --learning_rate 2e-3\n",
    "--seed 45 --learning_rate 2e-3\n",
    "--seed 42 --learning_rate 4e-3\n",
    "--seed 43 --learning_rate 4e-3\n",
    "--seed 44 --learning_rate 4e-3\n",
    "--seed 45 --learning_rate 4e-3\n",
    "--seed 42 --learning_rate 1e-2\n",
    "--seed 43 --learning_rate 1e-2\n",
    "--seed 44 --learning_rate 1e-2\n",
    "--seed 45 --learning_rate 1e-2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Lightning\n",
    "\n",
    "Writing the job arrays can be sometimes annoying, and hence it is adviced to write a script that can automatically generate the hyperparameter files (for instance by adding the seed parameter 4 times to each other hyperparam config). However, if you are using PyTorch Lightning, you can directly create a job array file. The documentation for this can be found [here](https://pytorch-lightning.readthedocs.io/en/latest/slurm.html#building-slurm-scripts)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
